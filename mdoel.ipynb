{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**GRU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn import linear_model\n",
    "# from d2l import torch as d2l\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rcz\\AppData\\Local\\Temp/ipykernel_36608/4069968976.py:1: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  BCHAIN_MKPRU=pd.read_csv(r\"C:\\Users\\rcz\\Desktop\\运筹学\\代码及数据\\1 数据预处理\\原始数据\\BCHAIN-MKPRU.csv\",dtype={\"Date\":np.str,\"Value\":np.float64})\n",
      "C:\\Users\\rcz\\AppData\\Local\\Temp/ipykernel_36608/4069968976.py:2: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  LBMA_GOLD=pd.read_csv(r\"C:\\Users\\rcz\\Desktop\\运筹学\\代码及数据\\1 数据预处理\\原始数据\\LBMA-GOLD.csv\",dtype={\"Date\":np.str,\"Value\":np.float64})\n"
     ]
    }
   ],
   "source": [
    "BCHAIN_MKPRU=pd.read_csv(r\"C:\\Users\\rcz\\Desktop\\运筹学\\代码及数据\\1 数据预处理\\原始数据\\BCHAIN-MKPRU.csv\",dtype={\"Date\":np.str,\"Value\":np.float64})\n",
    "LBMA_GOLD=pd.read_csv(r\"C:\\Users\\rcz\\Desktop\\运筹学\\代码及数据\\1 数据预处理\\原始数据\\LBMA-GOLD.csv\",dtype={\"Date\":np.str,\"Value\":np.float64})\n",
    "Data=pd.read_csv(r\"C:\\Users\\rcz\\Desktop\\运筹学\\代码及数据\\1 数据预处理\\处理后数据\\C题处理后的中间文件.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def to_timestamp(date):\n",
    "    return int(time.mktime(time.strptime(date,\"%m/%d/%y\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     日期(月/日/年)     比特币价值  是否可以买卖黄金     黄金价值\n",
      "0          0.0    621.65         1      NaN\n",
      "1          1.0    609.67         0  1324.60\n",
      "2          2.0    610.92         0  1323.65\n",
      "3          3.0    608.82         0  1321.75\n",
      "4          4.0    610.38         0  1310.80\n",
      "...        ...       ...       ...      ...\n",
      "1821    1821.0  51769.06         0  1821.60\n",
      "1822    1822.0  52677.40         0  1802.15\n",
      "1823    1823.0  46809.17         0  1786.00\n",
      "1824    1824.0  46078.38         0  1788.25\n",
      "1825    1825.0  46368.69         0  1794.60\n",
      "\n",
      "[1826 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#将日期变为自然数\n",
    "start_timestamp=to_timestamp(Data.iloc[0,0])\n",
    "for i in range(Data.shape[0]):\n",
    "    Data.iloc[i,0]=(to_timestamp(Data.iloc[i,0])-start_timestamp)/86400\n",
    "print(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size=1 # 应该只能为1\n",
    "start_input=30\n",
    "input_size=Data.shape[0] #训练：通过前input_size天预测input_size+1天，预测：通过2到input_size+1天预测第input_size+2天\n",
    "hidden_size=20\n",
    "# input_size=200\n",
    "output_size=1\n",
    "layers_size=3\n",
    "lr=10\n",
    "num_epochs=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, layers_size):\n",
    "        super().__init__()\n",
    "        self.GRU_layer = nn.GRU(input_size, hidden_size, layers_size)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.GRU_layer(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss0: 1788.4698486328125\n",
      "loss1: 1687.276123046875\n",
      "loss2: 1606.135498046875\n",
      "loss3: 1516.572998046875\n",
      "loss4: 1423.166748046875\n",
      "loss5: 1327.635498046875\n",
      "loss6: 1230.635498046875\n",
      "loss7: 1132.729248046875\n",
      "loss8: 1033.979248046875\n",
      "loss9: 934.854248046875\n",
      "loss10: 835.229248046875\n",
      "loss11: 735.354248046875\n",
      "loss12: 635.291748046875\n",
      "loss13: 534.854248046875\n",
      "loss14: 434.291748046875\n",
      "loss15: 333.916748046875\n",
      "loss16: 233.041748046875\n",
      "loss17: 132.416748046875\n",
      "loss18: 31.416748046875\n",
      "loss19: 69.708251953125\n",
      "loss20: 147.306396484375\n",
      "loss21: 204.66455078125\n",
      "loss22: 244.2774658203125\n",
      "loss23: 268.355712890625\n",
      "loss24: 278.83056640625\n",
      "loss25: 277.109619140625\n",
      "loss26: 264.704833984375\n",
      "loss27: 242.73583984375\n",
      "loss28: 212.4326171875\n",
      "loss29: 174.8873291015625\n",
      "loss30: 130.4310302734375\n",
      "loss31: 80.2603759765625\n",
      "loss32: 24.438720703125\n",
      "loss33: 35.477783203125\n",
      "loss34: 79.132080078125\n",
      "loss35: 107.8218994140625\n",
      "loss36: 123.4505615234375\n",
      "loss37: 127.1551513671875\n",
      "loss38: 119.8074951171875\n",
      "loss39: 103.3917236328125\n",
      "loss40: 77.7548828125\n",
      "loss41: 45.1092529296875\n",
      "loss42: 5.282470703125\n",
      "loss43: 40.9056396484375\n",
      "loss44: 72.123291015625\n",
      "loss45: 89.7701416015625\n",
      "loss46: 95.73046875\n",
      "loss47: 90.8746337890625\n",
      "loss48: 76.0609130859375\n",
      "loss49: 52.760986328125\n",
      "loss50: 21.937255859375\n",
      "loss51: 16.33251953125\n",
      "loss52: 40.219482421875\n",
      "loss53: 51.988037109375\n",
      "loss54: 52.263427734375\n",
      "loss55: 42.4083251953125\n",
      "loss56: 23.398681640625\n",
      "loss57: 3.674560546875\n",
      "loss58: 18.22509765625\n",
      "loss59: 21.0057373046875\n",
      "loss60: 13.5064697265625\n",
      "loss61: 3.5443115234375\n",
      "loss62: 8.67578125\n",
      "loss63: 3.13037109375\n",
      "loss64: 11.86181640625\n",
      "loss65: 15.203369140625\n",
      "loss66: 8.3848876953125\n",
      "loss67: 7.989990234375\n",
      "loss68: 12.70166015625\n",
      "loss69: 6.7413330078125\n",
      "loss70: 8.5361328125\n",
      "loss71: 12.2850341796875\n",
      "loss72: 5.62109375\n",
      "loss73: 10.351806640625\n",
      "loss74: 14.91455078125\n",
      "loss75: 8.80810546875\n",
      "loss76: 6.738037109375\n",
      "loss77: 10.7542724609375\n",
      "loss78: 4.10595703125\n",
      "loss79: 11.7276611328125\n",
      "loss80: 16.152587890625\n",
      "loss81: 9.909912109375\n",
      "loss82: 5.8961181640625\n",
      "loss83: 9.79638671875\n",
      "loss84: 3.5316162109375\n",
      "loss85: 12.54443359375\n",
      "loss86: 16.5877685546875\n",
      "loss87: 10.339111328125\n",
      "loss88: 5.2222900390625\n",
      "loss89: 9.37744140625\n",
      "loss90: 2.991943359375\n",
      "loss91: 12.7049560546875\n",
      "loss92: 16.99462890625\n",
      "loss93: 10.6177978515625\n",
      "loss94: 4.9462890625\n",
      "loss95: 9.10400390625\n",
      "loss96: 2.720947265625\n",
      "loss97: 12.848876953125\n",
      "loss98: 17.1365966796875\n",
      "loss99: 10.8831787109375\n",
      "loss100: 4.8076171875\n",
      "loss101: 8.9666748046875\n",
      "loss102: 2.5848388671875\n",
      "loss103: 12.98388671875\n",
      "loss104: 17.2706298828125\n",
      "loss105: 11.0162353515625\n",
      "loss106: 4.80029296875\n",
      "loss107: 8.8350830078125\n",
      "loss108: 2.578857421875\n",
      "loss109: 13.1142578125\n",
      "loss110: 17.400390625\n",
      "loss111: 11.0205078125\n",
      "loss112: 4.79638671875\n",
      "loss113: 8.83154296875\n",
      "loss114: 2.45068359375\n",
      "loss115: 13.1170654296875\n",
      "loss116: 17.403076171875\n",
      "loss117: 11.0228271484375\n",
      "loss118: 4.794189453125\n",
      "loss119: 8.8297119140625\n",
      "loss120: 2.448974609375\n",
      "loss121: 13.2435302734375\n",
      "loss122: 17.404296875\n",
      "loss123: 11.0240478515625\n",
      "loss124: 4.793212890625\n",
      "loss125: 8.8287353515625\n",
      "loss126: 2.4481201171875\n",
      "loss127: 13.244384765625\n",
      "loss128: 17.405029296875\n",
      "loss129: 11.024658203125\n",
      "loss130: 4.7926025390625\n",
      "loss131: 8.828125\n",
      "loss132: 2.44775390625\n",
      "loss133: 13.2447509765625\n",
      "loss134: 17.4053955078125\n",
      "loss135: 11.0250244140625\n",
      "loss136: 4.792236328125\n",
      "loss137: 8.827880859375\n",
      "loss138: 2.447509765625\n",
      "loss139: 13.2449951171875\n",
      "loss140: 17.4056396484375\n",
      "loss141: 11.0252685546875\n",
      "loss142: 4.7921142578125\n",
      "loss143: 8.8277587890625\n",
      "loss144: 2.447265625\n",
      "loss145: 13.2451171875\n",
      "loss146: 17.40576171875\n",
      "loss147: 11.0252685546875\n",
      "loss148: 4.7919921875\n",
      "loss149: 8.82763671875\n",
      "loss150: 2.447265625\n",
      "loss151: 13.2451171875\n",
      "loss152: 17.40576171875\n",
      "loss153: 11.025390625\n",
      "loss154: 4.7919921875\n",
      "loss155: 8.82763671875\n",
      "loss156: 2.447265625\n",
      "loss157: 13.2451171875\n",
      "loss158: 17.40576171875\n",
      "loss159: 11.025390625\n",
      "loss160: 4.7919921875\n",
      "loss161: 8.82763671875\n",
      "loss162: 2.4471435546875\n",
      "loss163: 13.2452392578125\n",
      "loss164: 17.4058837890625\n",
      "loss165: 11.025390625\n",
      "loss166: 4.7919921875\n",
      "loss167: 8.82763671875\n",
      "loss168: 2.4471435546875\n",
      "loss169: 13.2452392578125\n",
      "loss170: 17.4058837890625\n",
      "loss171: 11.025390625\n",
      "loss172: 4.7919921875\n",
      "loss173: 8.82763671875\n",
      "loss174: 2.4471435546875\n",
      "loss175: 13.2452392578125\n",
      "loss176: 17.4058837890625\n",
      "loss177: 11.025390625\n",
      "loss178: 4.7919921875\n",
      "loss179: 8.82763671875\n",
      "loss180: 2.4471435546875\n",
      "loss181: 13.2452392578125\n",
      "loss182: 17.4058837890625\n",
      "loss183: 11.025390625\n",
      "loss184: 4.7919921875\n",
      "loss185: 8.82763671875\n",
      "loss186: 2.4471435546875\n",
      "loss187: 13.2452392578125\n",
      "loss188: 17.4058837890625\n",
      "loss189: 11.025390625\n",
      "loss190: 4.7919921875\n",
      "loss191: 8.82763671875\n",
      "loss192: 2.4471435546875\n",
      "loss193: 13.2452392578125\n",
      "loss194: 17.4058837890625\n",
      "loss195: 11.025390625\n",
      "loss196: 4.7919921875\n",
      "loss197: 8.82763671875\n",
      "loss198: 2.4471435546875\n",
      "loss199: 13.2452392578125\n",
      "loss200: 17.4058837890625\n",
      "loss201: 11.025390625\n",
      "loss202: 4.7919921875\n",
      "loss203: 8.82763671875\n",
      "loss204: 2.4471435546875\n",
      "loss205: 13.2452392578125\n",
      "loss206: 17.4058837890625\n",
      "loss207: 11.025390625\n",
      "loss208: 4.7919921875\n",
      "loss209: 8.82763671875\n",
      "loss210: 2.4471435546875\n",
      "loss211: 13.2452392578125\n",
      "loss212: 17.4058837890625\n",
      "loss213: 11.025390625\n",
      "loss214: 4.7919921875\n",
      "loss215: 8.82763671875\n",
      "loss216: 2.4471435546875\n",
      "loss217: 13.2452392578125\n",
      "loss218: 17.4058837890625\n",
      "loss219: 11.025390625\n",
      "loss220: 4.7919921875\n",
      "loss221: 8.82763671875\n",
      "loss222: 2.4471435546875\n",
      "loss223: 13.2452392578125\n",
      "loss224: 17.4058837890625\n",
      "loss225: 11.025390625\n",
      "loss226: 4.7919921875\n",
      "loss227: 8.82763671875\n",
      "loss228: 2.4471435546875\n",
      "loss229: 13.2452392578125\n",
      "loss230: 17.4058837890625\n",
      "loss231: 11.025390625\n",
      "loss232: 4.7919921875\n",
      "loss233: 8.82763671875\n",
      "loss234: 2.4471435546875\n",
      "loss235: 13.2452392578125\n",
      "loss236: 17.4058837890625\n",
      "loss237: 11.025390625\n",
      "loss238: 4.7919921875\n",
      "loss239: 8.70263671875\n",
      "loss240: 2.4471435546875\n",
      "loss241: 13.2452392578125\n",
      "loss242: 17.4058837890625\n",
      "loss243: 11.025390625\n",
      "loss244: 4.7919921875\n",
      "loss245: 8.70263671875\n",
      "loss246: 2.4471435546875\n",
      "loss247: 13.2452392578125\n",
      "loss248: 17.4058837890625\n",
      "loss249: 11.025390625\n",
      "loss250: 4.7919921875\n",
      "loss251: 8.70263671875\n",
      "loss252: 2.4471435546875\n",
      "loss253: 13.2452392578125\n",
      "loss254: 17.4058837890625\n",
      "loss255: 11.025390625\n",
      "loss256: 4.7919921875\n",
      "loss257: 8.70263671875\n",
      "loss258: 2.4471435546875\n",
      "loss259: 13.2452392578125\n",
      "loss260: 17.4058837890625\n",
      "loss261: 11.025390625\n",
      "loss262: 4.7919921875\n",
      "loss263: 8.70263671875\n",
      "loss264: 2.4471435546875\n",
      "loss265: 13.2452392578125\n",
      "loss266: 17.4058837890625\n",
      "loss267: 11.025390625\n",
      "loss268: 4.7919921875\n",
      "loss269: 8.70263671875\n",
      "loss270: 2.4471435546875\n",
      "loss271: 13.2452392578125\n",
      "loss272: 17.4058837890625\n",
      "loss273: 11.025390625\n",
      "loss274: 4.7919921875\n",
      "loss275: 8.70263671875\n",
      "loss276: 2.4471435546875\n",
      "loss277: 13.2452392578125\n",
      "loss278: 17.4058837890625\n",
      "loss279: 11.025390625\n",
      "loss280: 4.7919921875\n",
      "loss281: 8.70263671875\n",
      "loss282: 2.4471435546875\n",
      "loss283: 13.2452392578125\n",
      "loss284: 17.4058837890625\n",
      "loss285: 11.025390625\n",
      "loss286: 4.7919921875\n",
      "loss287: 8.70263671875\n",
      "loss288: 2.4471435546875\n",
      "loss289: 13.2452392578125\n",
      "loss290: 17.4058837890625\n",
      "loss291: 11.025390625\n",
      "loss292: 4.7919921875\n",
      "loss293: 8.70263671875\n",
      "loss294: 2.4471435546875\n",
      "loss295: 13.2452392578125\n",
      "loss296: 17.4058837890625\n",
      "loss297: 11.025390625\n",
      "loss298: 4.7919921875\n",
      "loss299: 8.70263671875\n",
      "loss300: 2.4471435546875\n",
      "loss301: 13.2452392578125\n",
      "loss302: 17.4058837890625\n",
      "loss303: 11.025390625\n",
      "loss304: 4.7919921875\n",
      "loss305: 8.70263671875\n",
      "loss306: 2.4471435546875\n",
      "loss307: 13.2452392578125\n",
      "loss308: 17.4058837890625\n",
      "loss309: 11.025390625\n",
      "loss310: 4.7919921875\n",
      "loss311: 8.70263671875\n",
      "loss312: 2.4471435546875\n",
      "loss313: 13.2452392578125\n",
      "loss314: 17.4058837890625\n",
      "loss315: 11.025390625\n",
      "loss316: 4.7919921875\n",
      "loss317: 8.70263671875\n",
      "loss318: 2.4471435546875\n",
      "loss319: 13.2452392578125\n",
      "loss320: 17.4058837890625\n",
      "loss321: 11.025390625\n",
      "loss322: 4.7919921875\n",
      "loss323: 8.70263671875\n",
      "loss324: 2.4471435546875\n",
      "loss325: 13.2452392578125\n",
      "loss326: 17.4058837890625\n",
      "loss327: 11.025390625\n",
      "loss328: 4.7919921875\n",
      "loss329: 8.70263671875\n",
      "loss330: 2.4471435546875\n",
      "loss331: 13.1202392578125\n",
      "loss332: 17.4058837890625\n",
      "loss333: 11.025390625\n",
      "loss334: 4.7919921875\n",
      "loss335: 8.70263671875\n",
      "loss336: 2.4471435546875\n",
      "loss337: 13.1202392578125\n",
      "loss338: 17.4058837890625\n",
      "loss339: 11.025390625\n",
      "loss340: 4.7919921875\n",
      "loss341: 8.70263671875\n",
      "loss342: 2.4471435546875\n",
      "loss343: 13.1202392578125\n",
      "loss344: 17.4058837890625\n",
      "loss345: 11.025390625\n",
      "loss346: 4.7919921875\n",
      "loss347: 8.70263671875\n",
      "loss348: 2.4471435546875\n",
      "loss349: 13.1202392578125\n",
      "loss350: 17.4058837890625\n",
      "loss351: 11.025390625\n",
      "loss352: 4.7919921875\n",
      "loss353: 8.70263671875\n",
      "loss354: 2.4471435546875\n",
      "loss355: 13.1202392578125\n",
      "loss356: 17.4058837890625\n",
      "loss357: 11.025390625\n",
      "loss358: 4.7919921875\n",
      "loss359: 8.70263671875\n",
      "loss360: 2.4471435546875\n",
      "loss361: 13.1202392578125\n",
      "loss362: 17.4058837890625\n",
      "loss363: 11.025390625\n",
      "loss364: 4.7919921875\n",
      "loss365: 8.70263671875\n",
      "loss366: 2.4471435546875\n",
      "loss367: 13.1202392578125\n",
      "loss368: 17.4058837890625\n",
      "loss369: 11.025390625\n",
      "loss370: 4.7919921875\n",
      "loss371: 8.70263671875\n",
      "loss372: 2.4471435546875\n",
      "loss373: 13.1202392578125\n",
      "loss374: 17.4058837890625\n",
      "loss375: 11.025390625\n",
      "loss376: 4.7919921875\n",
      "loss377: 8.70263671875\n",
      "loss378: 2.4471435546875\n",
      "loss379: 13.1202392578125\n",
      "loss380: 17.4058837890625\n",
      "loss381: 11.025390625\n",
      "loss382: 4.7919921875\n",
      "loss383: 8.70263671875\n",
      "loss384: 2.4471435546875\n",
      "loss385: 13.1202392578125\n",
      "loss386: 17.4058837890625\n",
      "loss387: 11.025390625\n",
      "loss388: 4.7919921875\n",
      "loss389: 8.70263671875\n",
      "loss390: 2.4471435546875\n",
      "loss391: 13.1202392578125\n",
      "loss392: 17.4058837890625\n",
      "loss393: 11.025390625\n",
      "loss394: 4.7919921875\n",
      "loss395: 8.70263671875\n",
      "loss396: 2.4471435546875\n",
      "loss397: 13.1202392578125\n",
      "loss398: 17.4058837890625\n",
      "loss399: 11.025390625\n",
      "loss400: 4.7919921875\n",
      "loss401: 8.70263671875\n",
      "loss402: 2.4471435546875\n",
      "loss403: 13.1202392578125\n",
      "loss404: 17.4058837890625\n",
      "loss405: 11.025390625\n",
      "loss406: 4.7919921875\n",
      "loss407: 8.70263671875\n",
      "loss408: 2.4471435546875\n",
      "loss409: 13.1202392578125\n",
      "loss410: 17.4058837890625\n",
      "loss411: 11.025390625\n",
      "loss412: 4.7919921875\n",
      "loss413: 8.70263671875\n",
      "loss414: 2.4471435546875\n",
      "loss415: 13.1202392578125\n",
      "loss416: 17.4058837890625\n",
      "loss417: 11.025390625\n",
      "loss418: 4.7919921875\n",
      "loss419: 8.70263671875\n",
      "loss420: 2.4471435546875\n",
      "loss421: 13.1202392578125\n",
      "loss422: 17.4058837890625\n",
      "loss423: 11.025390625\n",
      "loss424: 4.7919921875\n",
      "loss425: 8.70263671875\n",
      "loss426: 2.4471435546875\n",
      "loss427: 13.1202392578125\n",
      "loss428: 17.4058837890625\n",
      "loss429: 11.025390625\n",
      "loss430: 4.7919921875\n",
      "loss431: 8.70263671875\n",
      "loss432: 2.4471435546875\n",
      "loss433: 13.1202392578125\n",
      "loss434: 17.4058837890625\n",
      "loss435: 11.025390625\n",
      "loss436: 4.7919921875\n",
      "loss437: 8.70263671875\n",
      "loss438: 2.4471435546875\n",
      "loss439: 13.1202392578125\n",
      "loss440: 17.4058837890625\n",
      "loss441: 11.025390625\n",
      "loss442: 4.7919921875\n",
      "loss443: 8.70263671875\n",
      "loss444: 2.4471435546875\n",
      "loss445: 13.1202392578125\n",
      "loss446: 17.4058837890625\n",
      "loss447: 11.025390625\n",
      "loss448: 4.7919921875\n",
      "loss449: 8.70263671875\n",
      "loss450: 2.4471435546875\n",
      "loss451: 13.1202392578125\n",
      "loss452: 17.4058837890625\n",
      "loss453: 11.025390625\n",
      "loss454: 4.7919921875\n",
      "loss455: 8.70263671875\n",
      "loss456: 2.4471435546875\n",
      "loss457: 13.1202392578125\n",
      "loss458: 17.4058837890625\n",
      "loss459: 11.025390625\n",
      "loss460: 4.7919921875\n",
      "loss461: 8.70263671875\n",
      "loss462: 2.4471435546875\n",
      "loss463: 13.1202392578125\n",
      "loss464: 17.4058837890625\n",
      "loss465: 11.025390625\n",
      "loss466: 4.7919921875\n",
      "loss467: 8.70263671875\n",
      "loss468: 2.4471435546875\n",
      "loss469: 13.1202392578125\n",
      "loss470: 17.4058837890625\n",
      "loss471: 11.025390625\n",
      "loss472: 4.7919921875\n",
      "loss473: 8.70263671875\n",
      "loss474: 2.4471435546875\n",
      "loss475: 13.1202392578125\n",
      "loss476: 17.4058837890625\n",
      "loss477: 11.025390625\n",
      "loss478: 4.7919921875\n",
      "loss479: 8.70263671875\n",
      "loss480: 2.4471435546875\n",
      "loss481: 13.1202392578125\n",
      "loss482: 17.4058837890625\n",
      "loss483: 11.025390625\n",
      "loss484: 4.7919921875\n",
      "loss485: 8.70263671875\n",
      "loss486: 2.4471435546875\n",
      "loss487: 13.1202392578125\n",
      "loss488: 17.4058837890625\n",
      "loss489: 11.025390625\n",
      "loss490: 4.7919921875\n",
      "loss491: 8.70263671875\n",
      "loss492: 2.4471435546875\n",
      "loss493: 13.1202392578125\n",
      "loss494: 17.4058837890625\n",
      "loss495: 11.025390625\n",
      "loss496: 4.7919921875\n",
      "loss497: 8.70263671875\n",
      "loss498: 2.4471435546875\n",
      "loss499: 13.1202392578125\n",
      "loss500: 17.4058837890625\n",
      "loss501: 11.025390625\n",
      "loss502: 4.7919921875\n",
      "loss503: 8.70263671875\n",
      "loss504: 2.4471435546875\n",
      "loss505: 13.1202392578125\n",
      "loss506: 17.4058837890625\n",
      "loss507: 11.025390625\n",
      "loss508: 4.7919921875\n",
      "loss509: 8.70263671875\n",
      "loss510: 2.4471435546875\n",
      "loss511: 13.1202392578125\n",
      "loss512: 17.4058837890625\n",
      "loss513: 11.025390625\n",
      "loss514: 4.7919921875\n",
      "loss515: 8.70263671875\n",
      "loss516: 2.4471435546875\n",
      "loss517: 13.1202392578125\n",
      "loss518: 17.4058837890625\n",
      "loss519: 11.025390625\n",
      "loss520: 4.7919921875\n",
      "loss521: 8.70263671875\n",
      "loss522: 2.4471435546875\n",
      "loss523: 13.1202392578125\n",
      "loss524: 17.4058837890625\n",
      "loss525: 11.025390625\n",
      "loss526: 4.7919921875\n",
      "loss527: 8.70263671875\n",
      "loss528: 2.4471435546875\n",
      "loss529: 13.1202392578125\n",
      "loss530: 17.4058837890625\n",
      "loss531: 11.025390625\n",
      "loss532: 4.7919921875\n",
      "loss533: 8.70263671875\n",
      "loss534: 2.4471435546875\n",
      "loss535: 13.1202392578125\n",
      "loss536: 17.4058837890625\n",
      "loss537: 11.025390625\n",
      "loss538: 4.7919921875\n",
      "loss539: 8.70263671875\n",
      "loss540: 2.4471435546875\n",
      "loss541: 13.1202392578125\n",
      "loss542: 17.4058837890625\n",
      "loss543: 11.025390625\n",
      "loss544: 4.7919921875\n",
      "loss545: 8.70263671875\n",
      "loss546: 2.4471435546875\n",
      "loss547: 13.1202392578125\n",
      "loss548: 17.4058837890625\n",
      "loss549: 11.025390625\n",
      "loss550: 4.7919921875\n",
      "loss551: 8.70263671875\n",
      "loss552: 2.4471435546875\n",
      "loss553: 13.1202392578125\n",
      "loss554: 17.4058837890625\n",
      "loss555: 11.025390625\n",
      "loss556: 4.7919921875\n",
      "loss557: 8.70263671875\n",
      "loss558: 2.4471435546875\n",
      "loss559: 13.1202392578125\n",
      "loss560: 17.4058837890625\n",
      "loss561: 11.025390625\n",
      "loss562: 4.7919921875\n",
      "loss563: 8.70263671875\n",
      "loss564: 2.4471435546875\n",
      "loss565: 13.1202392578125\n",
      "loss566: 17.4058837890625\n",
      "loss567: 11.025390625\n",
      "loss568: 4.7919921875\n",
      "loss569: 8.70263671875\n",
      "loss570: 2.4471435546875\n",
      "loss571: 13.1202392578125\n",
      "loss572: 17.4058837890625\n",
      "loss573: 11.025390625\n",
      "loss574: 4.7919921875\n",
      "loss575: 8.70263671875\n",
      "loss576: 2.4471435546875\n",
      "loss577: 13.1202392578125\n",
      "loss578: 17.4058837890625\n",
      "loss579: 11.025390625\n",
      "loss580: 4.7919921875\n",
      "loss581: 8.70263671875\n",
      "loss582: 2.4471435546875\n",
      "loss583: 13.1202392578125\n",
      "loss584: 17.4058837890625\n",
      "loss585: 11.025390625\n",
      "loss586: 4.7919921875\n",
      "loss587: 8.70263671875\n",
      "loss588: 2.4471435546875\n",
      "loss589: 13.1202392578125\n",
      "loss590: 17.4058837890625\n",
      "loss591: 11.025390625\n",
      "loss592: 4.7919921875\n",
      "loss593: 8.70263671875\n",
      "loss594: 2.4471435546875\n",
      "loss595: 13.1202392578125\n",
      "loss596: 17.4058837890625\n",
      "loss597: 11.025390625\n",
      "loss598: 4.7919921875\n",
      "loss599: 8.70263671875\n",
      "loss600: 2.4471435546875\n",
      "loss601: 13.1202392578125\n",
      "loss602: 17.4058837890625\n",
      "loss603: 11.025390625\n",
      "loss604: 4.7919921875\n",
      "loss605: 8.70263671875\n",
      "loss606: 2.4471435546875\n",
      "loss607: 13.1202392578125\n",
      "loss608: 17.4058837890625\n",
      "loss609: 11.025390625\n",
      "loss610: 4.7919921875\n",
      "loss611: 8.70263671875\n",
      "loss612: 2.4471435546875\n",
      "loss613: 13.1202392578125\n",
      "loss614: 17.4058837890625\n",
      "loss615: 11.025390625\n",
      "loss616: 4.7919921875\n",
      "loss617: 8.70263671875\n",
      "loss618: 2.4471435546875\n",
      "loss619: 13.1202392578125\n",
      "loss620: 17.4058837890625\n",
      "loss621: 11.025390625\n",
      "loss622: 4.7919921875\n",
      "loss623: 8.70263671875\n",
      "loss624: 2.4471435546875\n",
      "loss625: 13.1202392578125\n",
      "loss626: 17.4058837890625\n",
      "loss627: 11.025390625\n",
      "loss628: 4.7919921875\n",
      "loss629: 8.70263671875\n",
      "loss630: 2.4471435546875\n",
      "loss631: 13.1202392578125\n",
      "loss632: 17.4058837890625\n",
      "loss633: 11.025390625\n",
      "loss634: 4.7919921875\n",
      "loss635: 8.70263671875\n",
      "loss636: 2.4471435546875\n",
      "loss637: 13.1202392578125\n",
      "loss638: 17.4058837890625\n",
      "loss639: 11.025390625\n",
      "loss640: 4.7919921875\n",
      "loss641: 8.70263671875\n",
      "loss642: 2.4471435546875\n",
      "loss643: 13.1202392578125\n",
      "loss644: 17.4058837890625\n",
      "loss645: 11.025390625\n",
      "loss646: 4.7919921875\n",
      "loss647: 8.70263671875\n",
      "loss648: 2.4471435546875\n",
      "loss649: 13.1202392578125\n",
      "loss650: 17.4058837890625\n",
      "loss651: 11.025390625\n",
      "loss652: 4.7919921875\n",
      "loss653: 8.70263671875\n",
      "loss654: 2.4471435546875\n",
      "loss655: 13.1202392578125\n",
      "loss656: 17.4058837890625\n",
      "loss657: 11.025390625\n",
      "loss658: 4.7919921875\n",
      "loss659: 8.70263671875\n",
      "loss660: 2.4471435546875\n",
      "loss661: 13.1202392578125\n",
      "loss662: 17.4058837890625\n",
      "loss663: 11.025390625\n",
      "loss664: 4.7919921875\n",
      "loss665: 8.70263671875\n",
      "loss666: 2.4471435546875\n",
      "loss667: 13.1202392578125\n",
      "loss668: 17.4058837890625\n",
      "loss669: 11.025390625\n",
      "loss670: 4.7919921875\n",
      "loss671: 8.70263671875\n",
      "loss672: 2.4471435546875\n",
      "loss673: 13.1202392578125\n",
      "loss674: 17.4058837890625\n",
      "loss675: 11.025390625\n",
      "loss676: 4.7919921875\n",
      "loss677: 8.70263671875\n",
      "loss678: 2.4471435546875\n",
      "loss679: 13.1202392578125\n",
      "loss680: 17.4058837890625\n",
      "loss681: 11.025390625\n",
      "loss682: 4.7919921875\n",
      "loss683: 8.70263671875\n",
      "loss684: 2.4471435546875\n",
      "loss685: 13.1202392578125\n",
      "loss686: 17.4058837890625\n",
      "loss687: 11.025390625\n",
      "loss688: 4.7919921875\n",
      "loss689: 8.70263671875\n",
      "loss690: 2.4471435546875\n",
      "loss691: 13.1202392578125\n",
      "loss692: 17.4058837890625\n",
      "loss693: 11.025390625\n",
      "loss694: 4.7919921875\n",
      "loss695: 8.70263671875\n",
      "loss696: 2.4471435546875\n",
      "loss697: 13.1202392578125\n",
      "loss698: 17.4058837890625\n",
      "loss699: 11.025390625\n",
      "loss700: 4.7919921875\n",
      "loss701: 8.70263671875\n",
      "loss702: 2.4471435546875\n",
      "loss703: 13.1202392578125\n",
      "loss704: 17.4058837890625\n",
      "loss705: 11.025390625\n",
      "loss706: 4.7919921875\n",
      "loss707: 8.70263671875\n",
      "loss708: 2.4471435546875\n",
      "loss709: 13.1202392578125\n",
      "loss710: 17.4058837890625\n",
      "loss711: 11.025390625\n",
      "loss712: 4.7919921875\n",
      "loss713: 8.70263671875\n",
      "loss714: 2.4471435546875\n",
      "loss715: 13.1202392578125\n",
      "loss716: 17.4058837890625\n",
      "loss717: 11.025390625\n",
      "loss718: 4.7919921875\n",
      "loss719: 8.70263671875\n",
      "loss720: 2.4471435546875\n",
      "loss721: 13.1202392578125\n",
      "loss722: 17.4058837890625\n",
      "loss723: 11.025390625\n",
      "loss724: 4.7919921875\n",
      "loss725: 8.70263671875\n",
      "loss726: 2.4471435546875\n",
      "loss727: 13.1202392578125\n",
      "loss728: 17.4058837890625\n",
      "loss729: 11.025390625\n",
      "loss730: 4.7919921875\n",
      "loss731: 8.70263671875\n",
      "loss732: 2.4471435546875\n",
      "loss733: 13.1202392578125\n",
      "loss734: 17.4058837890625\n",
      "loss735: 11.025390625\n",
      "loss736: 4.7919921875\n",
      "loss737: 8.70263671875\n",
      "loss738: 2.4471435546875\n",
      "loss739: 13.1202392578125\n",
      "loss740: 17.4058837890625\n",
      "loss741: 11.025390625\n",
      "loss742: 4.7919921875\n",
      "loss743: 8.70263671875\n",
      "loss744: 2.4471435546875\n",
      "loss745: 13.1202392578125\n",
      "loss746: 17.4058837890625\n",
      "loss747: 11.025390625\n",
      "loss748: 4.7919921875\n",
      "loss749: 8.70263671875\n",
      "loss750: 2.4471435546875\n",
      "loss751: 13.1202392578125\n",
      "loss752: 17.4058837890625\n",
      "loss753: 11.025390625\n",
      "loss754: 4.7919921875\n",
      "loss755: 8.70263671875\n",
      "loss756: 2.4471435546875\n",
      "loss757: 13.1202392578125\n",
      "loss758: 17.4058837890625\n",
      "loss759: 11.025390625\n",
      "loss760: 4.7919921875\n",
      "loss761: 8.70263671875\n",
      "loss762: 2.4471435546875\n",
      "loss763: 13.1202392578125\n",
      "loss764: 17.4058837890625\n",
      "loss765: 11.025390625\n",
      "loss766: 4.7919921875\n",
      "loss767: 8.70263671875\n",
      "loss768: 2.4471435546875\n",
      "loss769: 13.1202392578125\n",
      "loss770: 17.4058837890625\n",
      "loss771: 11.025390625\n",
      "loss772: 4.7919921875\n",
      "loss773: 8.70263671875\n",
      "loss774: 2.4471435546875\n",
      "loss775: 13.1202392578125\n",
      "loss776: 17.4058837890625\n",
      "loss777: 11.025390625\n",
      "loss778: 4.7919921875\n",
      "loss779: 8.70263671875\n",
      "loss780: 2.4471435546875\n",
      "loss781: 13.1202392578125\n",
      "loss782: 17.4058837890625\n",
      "loss783: 11.025390625\n",
      "loss784: 4.7919921875\n",
      "loss785: 8.70263671875\n",
      "loss786: 2.4471435546875\n",
      "loss787: 13.1202392578125\n",
      "loss788: 17.4058837890625\n",
      "loss789: 11.025390625\n",
      "loss790: 4.7919921875\n",
      "loss791: 8.70263671875\n",
      "loss792: 2.4471435546875\n",
      "loss793: 13.1202392578125\n",
      "loss794: 17.4058837890625\n",
      "loss795: 11.025390625\n",
      "loss796: 4.7919921875\n",
      "loss797: 8.70263671875\n",
      "loss798: 2.4471435546875\n",
      "loss799: 13.1202392578125\n",
      "loss800: 17.4058837890625\n",
      "loss801: 11.025390625\n",
      "loss802: 4.7919921875\n",
      "loss803: 8.70263671875\n",
      "loss804: 2.4471435546875\n",
      "loss805: 13.1202392578125\n",
      "loss806: 17.4058837890625\n",
      "loss807: 11.025390625\n",
      "loss808: 4.7919921875\n",
      "loss809: 8.70263671875\n",
      "loss810: 2.4471435546875\n",
      "loss811: 13.1202392578125\n",
      "loss812: 17.4058837890625\n",
      "loss813: 11.025390625\n",
      "loss814: 4.7919921875\n",
      "loss815: 8.70263671875\n",
      "loss816: 2.4471435546875\n",
      "loss817: 13.1202392578125\n",
      "loss818: 17.4058837890625\n",
      "loss819: 11.025390625\n",
      "loss820: 4.7919921875\n",
      "loss821: 8.70263671875\n",
      "loss822: 2.4471435546875\n",
      "loss823: 13.1202392578125\n",
      "loss824: 17.4058837890625\n",
      "loss825: 11.025390625\n",
      "loss826: 4.7919921875\n",
      "loss827: 8.70263671875\n",
      "loss828: 2.4471435546875\n",
      "loss829: 13.1202392578125\n",
      "loss830: 17.4058837890625\n",
      "loss831: 11.025390625\n",
      "loss832: 4.7919921875\n",
      "loss833: 8.70263671875\n",
      "loss834: 2.4471435546875\n",
      "loss835: 13.1202392578125\n",
      "loss836: 17.4058837890625\n",
      "loss837: 11.025390625\n",
      "loss838: 4.7919921875\n",
      "loss839: 8.70263671875\n",
      "loss840: 2.4471435546875\n",
      "loss841: 13.1202392578125\n",
      "loss842: 17.4058837890625\n",
      "loss843: 11.025390625\n",
      "loss844: 4.7919921875\n",
      "loss845: 8.70263671875\n",
      "loss846: 2.4471435546875\n",
      "loss847: 13.1202392578125\n",
      "loss848: 17.4058837890625\n",
      "loss849: 11.025390625\n",
      "loss850: 4.7919921875\n",
      "loss851: 8.70263671875\n",
      "loss852: 2.4471435546875\n",
      "loss853: 13.1202392578125\n",
      "loss854: 17.4058837890625\n",
      "loss855: 11.025390625\n",
      "loss856: 4.7919921875\n",
      "loss857: 8.70263671875\n",
      "loss858: 2.4471435546875\n",
      "loss859: 13.1202392578125\n",
      "loss860: 17.4058837890625\n",
      "loss861: 11.025390625\n",
      "loss862: 4.7919921875\n",
      "loss863: 8.70263671875\n",
      "loss864: 2.4471435546875\n",
      "loss865: 13.1202392578125\n",
      "loss866: 17.4058837890625\n",
      "loss867: 11.025390625\n",
      "loss868: 4.7919921875\n",
      "loss869: 8.70263671875\n",
      "loss870: 2.4471435546875\n",
      "loss871: 13.1202392578125\n",
      "loss872: 17.4058837890625\n",
      "loss873: 11.025390625\n",
      "loss874: 4.7919921875\n",
      "loss875: 8.70263671875\n",
      "loss876: 2.4471435546875\n",
      "loss877: 13.1202392578125\n",
      "loss878: 17.4058837890625\n",
      "loss879: 11.025390625\n",
      "loss880: 4.7919921875\n",
      "loss881: 8.70263671875\n",
      "loss882: 2.4471435546875\n",
      "loss883: 13.1202392578125\n",
      "loss884: 17.4058837890625\n",
      "loss885: 11.025390625\n",
      "loss886: 4.7919921875\n",
      "loss887: 8.70263671875\n",
      "loss888: 2.4471435546875\n",
      "loss889: 13.1202392578125\n",
      "loss890: 17.4058837890625\n",
      "loss891: 11.025390625\n",
      "loss892: 4.7919921875\n",
      "loss893: 8.70263671875\n",
      "loss894: 2.4471435546875\n",
      "loss895: 13.1202392578125\n",
      "loss896: 17.4058837890625\n",
      "loss897: 11.025390625\n",
      "loss898: 4.7919921875\n",
      "loss899: 8.70263671875\n",
      "loss900: 2.4471435546875\n",
      "loss901: 13.1202392578125\n",
      "loss902: 17.4058837890625\n",
      "loss903: 11.025390625\n",
      "loss904: 4.7919921875\n",
      "loss905: 8.70263671875\n",
      "loss906: 2.4471435546875\n",
      "loss907: 13.1202392578125\n",
      "loss908: 17.4058837890625\n",
      "loss909: 11.025390625\n",
      "loss910: 4.7919921875\n",
      "loss911: 8.70263671875\n",
      "loss912: 2.4471435546875\n",
      "loss913: 13.1202392578125\n",
      "loss914: 17.4058837890625\n",
      "loss915: 11.025390625\n",
      "loss916: 4.7919921875\n",
      "loss917: 8.70263671875\n",
      "loss918: 2.4471435546875\n",
      "loss919: 13.1202392578125\n",
      "loss920: 17.4058837890625\n",
      "loss921: 11.025390625\n",
      "loss922: 4.7919921875\n",
      "loss923: 8.70263671875\n",
      "loss924: 2.4471435546875\n",
      "loss925: 13.1202392578125\n",
      "loss926: 17.4058837890625\n",
      "loss927: 11.025390625\n",
      "loss928: 4.7919921875\n",
      "loss929: 8.70263671875\n",
      "loss930: 2.4471435546875\n",
      "loss931: 13.1202392578125\n",
      "loss932: 17.4058837890625\n",
      "loss933: 11.025390625\n",
      "loss934: 4.7919921875\n",
      "loss935: 8.70263671875\n",
      "loss936: 2.4471435546875\n",
      "loss937: 13.1202392578125\n",
      "loss938: 17.4058837890625\n",
      "loss939: 11.025390625\n",
      "loss940: 4.7919921875\n",
      "loss941: 8.70263671875\n",
      "loss942: 2.4471435546875\n",
      "loss943: 13.1202392578125\n",
      "loss944: 17.4058837890625\n",
      "loss945: 11.025390625\n",
      "loss946: 4.7919921875\n",
      "loss947: 8.70263671875\n",
      "loss948: 2.4471435546875\n",
      "loss949: 13.1202392578125\n",
      "loss950: 17.4058837890625\n",
      "loss951: 11.025390625\n",
      "loss952: 4.7919921875\n",
      "loss953: 8.70263671875\n",
      "loss954: 2.4471435546875\n",
      "loss955: 13.1202392578125\n",
      "loss956: 17.4058837890625\n",
      "loss957: 11.025390625\n",
      "loss958: 4.7919921875\n",
      "loss959: 8.70263671875\n",
      "loss960: 2.4471435546875\n",
      "loss961: 13.1202392578125\n",
      "loss962: 17.4058837890625\n",
      "loss963: 11.025390625\n",
      "loss964: 4.7919921875\n",
      "loss965: 8.70263671875\n",
      "loss966: 2.4471435546875\n",
      "loss967: 13.1202392578125\n",
      "loss968: 17.4058837890625\n",
      "loss969: 11.025390625\n",
      "loss970: 4.7919921875\n",
      "loss971: 8.70263671875\n",
      "loss972: 2.4471435546875\n",
      "loss973: 13.1202392578125\n",
      "loss974: 17.4058837890625\n",
      "loss975: 11.025390625\n",
      "loss976: 4.7919921875\n",
      "loss977: 8.70263671875\n",
      "loss978: 2.4471435546875\n",
      "loss979: 13.1202392578125\n",
      "loss980: 17.4058837890625\n",
      "loss981: 11.025390625\n",
      "loss982: 4.7919921875\n",
      "loss983: 8.70263671875\n",
      "loss984: 2.4471435546875\n",
      "loss985: 13.1202392578125\n",
      "loss986: 17.4058837890625\n",
      "loss987: 11.025390625\n",
      "loss988: 4.7919921875\n",
      "loss989: 8.70263671875\n",
      "loss990: 2.4471435546875\n",
      "loss991: 13.1202392578125\n",
      "loss992: 17.4058837890625\n",
      "loss993: 11.025390625\n",
      "loss994: 4.7919921875\n",
      "loss995: 8.70263671875\n",
      "loss996: 2.4471435546875\n",
      "loss997: 13.1202392578125\n",
      "loss998: 17.4058837890625\n",
      "loss999: 11.025390625\n"
     ]
    }
   ],
   "source": [
    "# 训练过程\n",
    "device=torch.device(\"cuda\")\n",
    "\n",
    "gru=GRUModel(30, hidden_size, output_size, layers_size).to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(gru.parameters(), lr)\n",
    "\n",
    "ji=np.array(Data.iloc[0:input_size,3].dropna())\n",
    "input_size=ji.shape[0]-2\n",
    "\n",
    "trainB_x=torch.from_numpy(ji[input_size-30:input_size].reshape(-1,batch_size,30)).to(torch.float32).to(device)\n",
    "trainB_y=torch.from_numpy(ji[input_size].reshape(-1,batch_size,output_size)).to(torch.float32).to(device)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    output = gru(trainB_x).to(device)\n",
    "    loss = criterion(output, trainB_y)\n",
    "    losses.append(loss)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"loss\" + str(epoch) + \":\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: 1793.0419921875\n",
      "actual: 1794.6\n"
     ]
    }
   ],
   "source": [
    "# 预测，以比特币为例\n",
    "# pred_x_train=torch.from_numpy(np.array(Data.iloc[1:input_size+1,1]).reshape(-1,1,input_size)).to(torch.float32).to(device)\n",
    "pred_x_train=torch.from_numpy(ji[input_size-29:input_size+1]).reshape(-1,1,30).to(torch.float32).to(device)\n",
    "pred_y_train=gru(pred_x_train).to(device)\n",
    "print(\"prediction:\",pred_y_train.item())\n",
    "print(\"actual:\",ji[input_size+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "进行到input_size= 30\n",
      "进行到input_size= 31\n",
      "进行到input_size= 32\n",
      "进行到input_size= 33\n",
      "进行到input_size= 34\n",
      "进行到input_size= 35\n",
      "进行到input_size= 36\n",
      "进行到input_size= 37\n",
      "进行到input_size= 38\n",
      "进行到input_size= 39\n",
      "进行到input_size= 40\n",
      "进行到input_size= 41\n",
      "进行到input_size= 42\n",
      "进行到input_size= 43\n",
      "进行到input_size= 44\n",
      "进行到input_size= 45\n",
      "进行到input_size= 46\n",
      "进行到input_size= 47\n",
      "进行到input_size= 48\n",
      "进行到input_size= 49\n",
      "进行到input_size= 50\n",
      "进行到input_size= 51\n",
      "进行到input_size= 52\n",
      "进行到input_size= 53\n",
      "进行到input_size= 54\n",
      "进行到input_size= 55\n",
      "进行到input_size= 56\n",
      "进行到input_size= 57\n",
      "进行到input_size= 58\n",
      "进行到input_size= 59\n",
      "进行到input_size= 60\n",
      "进行到input_size= 61\n",
      "进行到input_size= 62\n",
      "进行到input_size= 63\n",
      "进行到input_size= 64\n",
      "进行到input_size= 65\n",
      "进行到input_size= 66\n",
      "进行到input_size= 67\n",
      "进行到input_size= 68\n",
      "进行到input_size= 69\n",
      "进行到input_size= 70\n",
      "进行到input_size= 71\n",
      "进行到input_size= 72\n",
      "进行到input_size= 73\n",
      "进行到input_size= 74\n",
      "进行到input_size= 75\n",
      "进行到input_size= 76\n",
      "进行到input_size= 77\n",
      "进行到input_size= 78\n",
      "进行到input_size= 79\n",
      "进行到input_size= 80\n",
      "进行到input_size= 81\n",
      "进行到input_size= 82\n",
      "进行到input_size= 83\n",
      "进行到input_size= 84\n",
      "进行到input_size= 85\n",
      "进行到input_size= 86\n",
      "进行到input_size= 87\n",
      "进行到input_size= 88\n",
      "进行到input_size= 89\n",
      "进行到input_size= 90\n",
      "进行到input_size= 91\n",
      "进行到input_size= 92\n",
      "进行到input_size= 93\n",
      "进行到input_size= 94\n",
      "进行到input_size= 95\n",
      "进行到input_size= 96\n",
      "进行到input_size= 97\n",
      "进行到input_size= 98\n",
      "进行到input_size= 99\n",
      "进行到input_size= 100\n",
      "进行到input_size= 101\n",
      "进行到input_size= 102\n",
      "进行到input_size= 103\n",
      "进行到input_size= 104\n",
      "进行到input_size= 105\n",
      "进行到input_size= 106\n",
      "进行到input_size= 107\n",
      "进行到input_size= 108\n",
      "进行到input_size= 109\n",
      "进行到input_size= 110\n",
      "进行到input_size= 111\n",
      "进行到input_size= 112\n",
      "进行到input_size= 113\n",
      "进行到input_size= 114\n",
      "进行到input_size= 115\n",
      "进行到input_size= 116\n",
      "进行到input_size= 117\n",
      "进行到input_size= 118\n",
      "进行到input_size= 119\n",
      "进行到input_size= 120\n",
      "进行到input_size= 121\n",
      "进行到input_size= 122\n",
      "进行到input_size= 123\n",
      "进行到input_size= 124\n",
      "进行到input_size= 125\n",
      "进行到input_size= 126\n",
      "进行到input_size= 127\n",
      "进行到input_size= 128\n",
      "进行到input_size= 129\n",
      "进行到input_size= 130\n",
      "进行到input_size= 131\n",
      "进行到input_size= 132\n",
      "进行到input_size= 133\n",
      "进行到input_size= 134\n",
      "进行到input_size= 135\n",
      "进行到input_size= 136\n",
      "进行到input_size= 137\n",
      "进行到input_size= 138\n",
      "进行到input_size= 139\n",
      "进行到input_size= 140\n",
      "进行到input_size= 141\n",
      "进行到input_size= 142\n",
      "进行到input_size= 143\n",
      "进行到input_size= 144\n",
      "进行到input_size= 145\n",
      "进行到input_size= 146\n",
      "进行到input_size= 147\n",
      "进行到input_size= 148\n",
      "进行到input_size= 149\n",
      "进行到input_size= 150\n",
      "进行到input_size= 151\n",
      "进行到input_size= 152\n",
      "进行到input_size= 153\n",
      "进行到input_size= 154\n",
      "进行到input_size= 155\n",
      "进行到input_size= 156\n",
      "进行到input_size= 157\n",
      "进行到input_size= 158\n",
      "进行到input_size= 159\n",
      "进行到input_size= 160\n",
      "进行到input_size= 161\n",
      "进行到input_size= 162\n",
      "进行到input_size= 163\n",
      "进行到input_size= 164\n",
      "进行到input_size= 165\n",
      "进行到input_size= 166\n",
      "进行到input_size= 167\n",
      "进行到input_size= 168\n",
      "进行到input_size= 169\n",
      "进行到input_size= 170\n",
      "进行到input_size= 171\n",
      "进行到input_size= 172\n",
      "进行到input_size= 173\n",
      "进行到input_size= 174\n",
      "进行到input_size= 175\n",
      "进行到input_size= 176\n",
      "进行到input_size= 177\n",
      "进行到input_size= 178\n",
      "进行到input_size= 179\n",
      "进行到input_size= 180\n",
      "进行到input_size= 181\n",
      "进行到input_size= 182\n",
      "进行到input_size= 183\n",
      "进行到input_size= 184\n",
      "进行到input_size= 185\n",
      "进行到input_size= 186\n",
      "进行到input_size= 187\n",
      "进行到input_size= 188\n",
      "进行到input_size= 189\n",
      "进行到input_size= 190\n",
      "进行到input_size= 191\n",
      "进行到input_size= 192\n",
      "进行到input_size= 193\n",
      "进行到input_size= 194\n",
      "进行到input_size= 195\n",
      "进行到input_size= 196\n",
      "进行到input_size= 197\n",
      "进行到input_size= 198\n",
      "进行到input_size= 199\n",
      "进行到input_size= 200\n",
      "进行到input_size= 201\n",
      "进行到input_size= 202\n",
      "进行到input_size= 203\n",
      "进行到input_size= 204\n",
      "进行到input_size= 205\n",
      "进行到input_size= 206\n",
      "进行到input_size= 207\n",
      "进行到input_size= 208\n",
      "进行到input_size= 209\n",
      "进行到input_size= 210\n",
      "进行到input_size= 211\n",
      "进行到input_size= 212\n",
      "进行到input_size= 213\n",
      "进行到input_size= 214\n",
      "进行到input_size= 215\n",
      "进行到input_size= 216\n",
      "进行到input_size= 217\n",
      "进行到input_size= 218\n",
      "进行到input_size= 219\n",
      "进行到input_size= 220\n",
      "进行到input_size= 221\n",
      "进行到input_size= 222\n",
      "进行到input_size= 223\n",
      "进行到input_size= 224\n",
      "进行到input_size= 225\n",
      "进行到input_size= 226\n",
      "进行到input_size= 227\n",
      "进行到input_size= 228\n",
      "进行到input_size= 229\n",
      "进行到input_size= 230\n",
      "进行到input_size= 231\n",
      "进行到input_size= 232\n",
      "进行到input_size= 233\n",
      "进行到input_size= 234\n",
      "进行到input_size= 235\n",
      "进行到input_size= 236\n",
      "进行到input_size= 237\n",
      "进行到input_size= 238\n",
      "进行到input_size= 239\n",
      "进行到input_size= 240\n",
      "进行到input_size= 241\n",
      "进行到input_size= 242\n",
      "进行到input_size= 243\n",
      "进行到input_size= 244\n",
      "进行到input_size= 245\n",
      "进行到input_size= 246\n",
      "进行到input_size= 247\n",
      "进行到input_size= 248\n",
      "进行到input_size= 249\n",
      "进行到input_size= 250\n",
      "进行到input_size= 251\n",
      "进行到input_size= 252\n",
      "进行到input_size= 253\n",
      "进行到input_size= 254\n",
      "进行到input_size= 255\n",
      "进行到input_size= 256\n",
      "进行到input_size= 257\n",
      "进行到input_size= 258\n",
      "进行到input_size= 259\n",
      "进行到input_size= 260\n",
      "进行到input_size= 261\n",
      "进行到input_size= 262\n",
      "进行到input_size= 263\n",
      "进行到input_size= 264\n",
      "进行到input_size= 265\n",
      "进行到input_size= 266\n",
      "进行到input_size= 267\n",
      "进行到input_size= 268\n",
      "进行到input_size= 269\n",
      "进行到input_size= 270\n",
      "进行到input_size= 271\n",
      "进行到input_size= 272\n",
      "进行到input_size= 273\n",
      "进行到input_size= 274\n",
      "进行到input_size= 275\n",
      "进行到input_size= 276\n",
      "进行到input_size= 277\n",
      "进行到input_size= 278\n",
      "进行到input_size= 279\n",
      "进行到input_size= 280\n",
      "进行到input_size= 281\n",
      "进行到input_size= 282\n",
      "进行到input_size= 283\n",
      "进行到input_size= 284\n",
      "进行到input_size= 285\n",
      "进行到input_size= 286\n",
      "进行到input_size= 287\n",
      "进行到input_size= 288\n",
      "进行到input_size= 289\n",
      "进行到input_size= 290\n",
      "进行到input_size= 291\n",
      "进行到input_size= 292\n",
      "进行到input_size= 293\n",
      "进行到input_size= 294\n",
      "进行到input_size= 295\n",
      "进行到input_size= 296\n",
      "进行到input_size= 297\n",
      "进行到input_size= 298\n",
      "进行到input_size= 299\n",
      "进行到input_size= 300\n",
      "进行到input_size= 301\n",
      "进行到input_size= 302\n",
      "进行到input_size= 303\n",
      "进行到input_size= 304\n",
      "进行到input_size= 305\n",
      "进行到input_size= 306\n",
      "进行到input_size= 307\n",
      "进行到input_size= 308\n",
      "进行到input_size= 309\n",
      "进行到input_size= 310\n",
      "进行到input_size= 311\n",
      "进行到input_size= 312\n",
      "进行到input_size= 313\n",
      "进行到input_size= 314\n",
      "进行到input_size= 315\n",
      "进行到input_size= 316\n",
      "进行到input_size= 317\n",
      "进行到input_size= 318\n",
      "进行到input_size= 319\n",
      "进行到input_size= 320\n",
      "进行到input_size= 321\n",
      "进行到input_size= 322\n",
      "进行到input_size= 323\n",
      "进行到input_size= 324\n",
      "进行到input_size= 325\n",
      "进行到input_size= 326\n",
      "进行到input_size= 327\n",
      "进行到input_size= 328\n",
      "进行到input_size= 329\n",
      "进行到input_size= 330\n",
      "进行到input_size= 331\n",
      "进行到input_size= 332\n",
      "进行到input_size= 333\n",
      "进行到input_size= 334\n",
      "进行到input_size= 335\n",
      "进行到input_size= 336\n",
      "进行到input_size= 337\n",
      "进行到input_size= 338\n",
      "进行到input_size= 339\n",
      "进行到input_size= 340\n",
      "进行到input_size= 341\n",
      "进行到input_size= 342\n",
      "进行到input_size= 343\n",
      "进行到input_size= 344\n",
      "进行到input_size= 345\n",
      "进行到input_size= 346\n",
      "进行到input_size= 347\n",
      "进行到input_size= 348\n",
      "进行到input_size= 349\n",
      "进行到input_size= 350\n",
      "进行到input_size= 351\n",
      "进行到input_size= 352\n",
      "进行到input_size= 353\n",
      "进行到input_size= 354\n",
      "进行到input_size= 355\n",
      "进行到input_size= 356\n",
      "进行到input_size= 357\n",
      "进行到input_size= 358\n",
      "进行到input_size= 359\n",
      "进行到input_size= 360\n",
      "进行到input_size= 361\n",
      "进行到input_size= 362\n",
      "进行到input_size= 363\n",
      "进行到input_size= 364\n",
      "进行到input_size= 365\n",
      "进行到input_size= 366\n",
      "进行到input_size= 367\n",
      "进行到input_size= 368\n",
      "进行到input_size= 369\n",
      "进行到input_size= 370\n",
      "进行到input_size= 371\n",
      "进行到input_size= 372\n",
      "进行到input_size= 373\n",
      "进行到input_size= 374\n",
      "进行到input_size= 375\n",
      "进行到input_size= 376\n",
      "进行到input_size= 377\n",
      "进行到input_size= 378\n",
      "进行到input_size= 379\n",
      "进行到input_size= 380\n",
      "进行到input_size= 381\n",
      "进行到input_size= 382\n",
      "进行到input_size= 383\n",
      "进行到input_size= 384\n",
      "进行到input_size= 385\n",
      "进行到input_size= 386\n",
      "进行到input_size= 387\n",
      "进行到input_size= 388\n",
      "进行到input_size= 389\n",
      "进行到input_size= 390\n",
      "进行到input_size= 391\n",
      "进行到input_size= 392\n",
      "进行到input_size= 393\n",
      "进行到input_size= 394\n",
      "进行到input_size= 395\n",
      "进行到input_size= 396\n",
      "进行到input_size= 397\n",
      "进行到input_size= 398\n",
      "进行到input_size= 399\n",
      "进行到input_size= 400\n",
      "进行到input_size= 401\n",
      "进行到input_size= 402\n",
      "进行到input_size= 403\n",
      "进行到input_size= 404\n",
      "进行到input_size= 405\n",
      "进行到input_size= 406\n",
      "进行到input_size= 407\n",
      "进行到input_size= 408\n",
      "进行到input_size= 409\n",
      "进行到input_size= 410\n",
      "进行到input_size= 411\n",
      "进行到input_size= 412\n",
      "进行到input_size= 413\n",
      "进行到input_size= 414\n",
      "进行到input_size= 415\n",
      "进行到input_size= 416\n",
      "进行到input_size= 417\n",
      "进行到input_size= 418\n",
      "进行到input_size= 419\n",
      "进行到input_size= 420\n",
      "进行到input_size= 421\n",
      "进行到input_size= 422\n",
      "进行到input_size= 423\n",
      "进行到input_size= 424\n",
      "进行到input_size= 425\n",
      "进行到input_size= 426\n",
      "进行到input_size= 427\n",
      "进行到input_size= 428\n",
      "进行到input_size= 429\n",
      "进行到input_size= 430\n",
      "进行到input_size= 431\n",
      "进行到input_size= 432\n",
      "进行到input_size= 433\n",
      "进行到input_size= 434\n",
      "进行到input_size= 435\n",
      "进行到input_size= 436\n",
      "进行到input_size= 437\n",
      "进行到input_size= 438\n",
      "进行到input_size= 439\n",
      "进行到input_size= 440\n",
      "进行到input_size= 441\n",
      "进行到input_size= 442\n",
      "进行到input_size= 443\n",
      "进行到input_size= 444\n",
      "进行到input_size= 445\n",
      "进行到input_size= 446\n",
      "进行到input_size= 447\n",
      "进行到input_size= 448\n",
      "进行到input_size= 449\n",
      "进行到input_size= 450\n",
      "进行到input_size= 451\n",
      "进行到input_size= 452\n",
      "进行到input_size= 453\n",
      "进行到input_size= 454\n",
      "进行到input_size= 455\n",
      "进行到input_size= 456\n",
      "进行到input_size= 457\n",
      "进行到input_size= 458\n",
      "进行到input_size= 459\n",
      "进行到input_size= 460\n",
      "进行到input_size= 461\n",
      "进行到input_size= 462\n",
      "进行到input_size= 463\n",
      "进行到input_size= 464\n",
      "进行到input_size= 465\n",
      "进行到input_size= 466\n",
      "进行到input_size= 467\n",
      "进行到input_size= 468\n",
      "进行到input_size= 469\n",
      "进行到input_size= 470\n",
      "进行到input_size= 471\n",
      "进行到input_size= 472\n",
      "进行到input_size= 473\n",
      "进行到input_size= 474\n",
      "进行到input_size= 475\n",
      "进行到input_size= 476\n",
      "进行到input_size= 477\n",
      "进行到input_size= 478\n",
      "进行到input_size= 479\n",
      "进行到input_size= 480\n",
      "进行到input_size= 481\n",
      "进行到input_size= 482\n",
      "进行到input_size= 483\n",
      "进行到input_size= 484\n",
      "进行到input_size= 485\n",
      "进行到input_size= 486\n",
      "进行到input_size= 487\n",
      "进行到input_size= 488\n",
      "进行到input_size= 489\n",
      "进行到input_size= 490\n",
      "进行到input_size= 491\n",
      "进行到input_size= 492\n",
      "进行到input_size= 493\n",
      "进行到input_size= 494\n",
      "进行到input_size= 495\n",
      "进行到input_size= 496\n",
      "进行到input_size= 497\n",
      "进行到input_size= 498\n",
      "进行到input_size= 499\n",
      "进行到input_size= 500\n",
      "进行到input_size= 501\n",
      "进行到input_size= 502\n",
      "进行到input_size= 503\n",
      "进行到input_size= 504\n",
      "进行到input_size= 505\n",
      "进行到input_size= 506\n",
      "进行到input_size= 507\n",
      "进行到input_size= 508\n",
      "进行到input_size= 509\n",
      "进行到input_size= 510\n",
      "进行到input_size= 511\n",
      "进行到input_size= 512\n",
      "进行到input_size= 513\n",
      "进行到input_size= 514\n",
      "进行到input_size= 515\n",
      "进行到input_size= 516\n",
      "进行到input_size= 517\n",
      "进行到input_size= 518\n",
      "进行到input_size= 519\n",
      "进行到input_size= 520\n",
      "进行到input_size= 521\n",
      "进行到input_size= 522\n",
      "进行到input_size= 523\n",
      "进行到input_size= 524\n",
      "进行到input_size= 525\n",
      "进行到input_size= 526\n",
      "进行到input_size= 527\n",
      "进行到input_size= 528\n",
      "进行到input_size= 529\n",
      "进行到input_size= 530\n",
      "进行到input_size= 531\n",
      "进行到input_size= 532\n",
      "进行到input_size= 533\n",
      "进行到input_size= 534\n",
      "进行到input_size= 535\n",
      "进行到input_size= 536\n",
      "进行到input_size= 537\n",
      "进行到input_size= 538\n",
      "进行到input_size= 539\n",
      "进行到input_size= 540\n",
      "进行到input_size= 541\n",
      "进行到input_size= 542\n",
      "进行到input_size= 543\n",
      "进行到input_size= 544\n",
      "进行到input_size= 545\n",
      "进行到input_size= 546\n",
      "进行到input_size= 547\n",
      "进行到input_size= 548\n",
      "进行到input_size= 549\n",
      "进行到input_size= 550\n",
      "进行到input_size= 551\n",
      "进行到input_size= 552\n",
      "进行到input_size= 553\n",
      "进行到input_size= 554\n",
      "进行到input_size= 555\n",
      "进行到input_size= 556\n",
      "进行到input_size= 557\n",
      "进行到input_size= 558\n",
      "进行到input_size= 559\n",
      "进行到input_size= 560\n",
      "进行到input_size= 561\n",
      "进行到input_size= 562\n",
      "进行到input_size= 563\n",
      "进行到input_size= 564\n",
      "进行到input_size= 565\n",
      "进行到input_size= 566\n",
      "进行到input_size= 567\n",
      "进行到input_size= 568\n",
      "进行到input_size= 569\n",
      "进行到input_size= 570\n",
      "进行到input_size= 571\n",
      "进行到input_size= 572\n",
      "进行到input_size= 573\n",
      "进行到input_size= 574\n",
      "进行到input_size= 575\n",
      "进行到input_size= 576\n",
      "进行到input_size= 577\n",
      "进行到input_size= 578\n",
      "进行到input_size= 579\n",
      "进行到input_size= 580\n",
      "进行到input_size= 581\n",
      "进行到input_size= 582\n",
      "进行到input_size= 583\n",
      "进行到input_size= 584\n",
      "进行到input_size= 585\n",
      "进行到input_size= 586\n",
      "进行到input_size= 587\n",
      "进行到input_size= 588\n",
      "进行到input_size= 589\n",
      "进行到input_size= 590\n",
      "进行到input_size= 591\n",
      "进行到input_size= 592\n",
      "进行到input_size= 593\n",
      "进行到input_size= 594\n",
      "进行到input_size= 595\n",
      "进行到input_size= 596\n",
      "进行到input_size= 597\n",
      "进行到input_size= 598\n",
      "进行到input_size= 599\n",
      "进行到input_size= 600\n",
      "进行到input_size= 601\n",
      "进行到input_size= 602\n",
      "进行到input_size= 603\n",
      "进行到input_size= 604\n",
      "进行到input_size= 605\n",
      "进行到input_size= 606\n",
      "进行到input_size= 607\n",
      "进行到input_size= 608\n",
      "进行到input_size= 609\n",
      "进行到input_size= 610\n",
      "进行到input_size= 611\n",
      "进行到input_size= 612\n",
      "进行到input_size= 613\n",
      "进行到input_size= 614\n",
      "进行到input_size= 615\n",
      "进行到input_size= 616\n",
      "进行到input_size= 617\n",
      "进行到input_size= 618\n",
      "进行到input_size= 619\n",
      "进行到input_size= 620\n",
      "进行到input_size= 621\n",
      "进行到input_size= 622\n",
      "进行到input_size= 623\n",
      "进行到input_size= 624\n",
      "进行到input_size= 625\n",
      "进行到input_size= 626\n",
      "进行到input_size= 627\n",
      "进行到input_size= 628\n",
      "进行到input_size= 629\n",
      "进行到input_size= 630\n",
      "进行到input_size= 631\n",
      "进行到input_size= 632\n",
      "进行到input_size= 633\n",
      "进行到input_size= 634\n",
      "进行到input_size= 635\n",
      "进行到input_size= 636\n",
      "进行到input_size= 637\n",
      "进行到input_size= 638\n",
      "进行到input_size= 639\n",
      "进行到input_size= 640\n",
      "进行到input_size= 641\n",
      "进行到input_size= 642\n",
      "进行到input_size= 643\n",
      "进行到input_size= 644\n",
      "进行到input_size= 645\n",
      "进行到input_size= 646\n",
      "进行到input_size= 647\n",
      "进行到input_size= 648\n",
      "进行到input_size= 649\n",
      "进行到input_size= 650\n",
      "进行到input_size= 651\n",
      "进行到input_size= 652\n",
      "进行到input_size= 653\n",
      "进行到input_size= 654\n",
      "进行到input_size= 655\n",
      "进行到input_size= 656\n",
      "进行到input_size= 657\n",
      "进行到input_size= 658\n",
      "进行到input_size= 659\n",
      "进行到input_size= 660\n",
      "进行到input_size= 661\n",
      "进行到input_size= 662\n",
      "进行到input_size= 663\n",
      "进行到input_size= 664\n",
      "进行到input_size= 665\n",
      "进行到input_size= 666\n",
      "进行到input_size= 667\n",
      "进行到input_size= 668\n",
      "进行到input_size= 669\n",
      "进行到input_size= 670\n",
      "进行到input_size= 671\n",
      "进行到input_size= 672\n",
      "进行到input_size= 673\n",
      "进行到input_size= 674\n",
      "进行到input_size= 675\n",
      "进行到input_size= 676\n",
      "进行到input_size= 677\n",
      "进行到input_size= 678\n",
      "进行到input_size= 679\n",
      "进行到input_size= 680\n",
      "进行到input_size= 681\n",
      "进行到input_size= 682\n",
      "进行到input_size= 683\n",
      "进行到input_size= 684\n",
      "进行到input_size= 685\n",
      "进行到input_size= 686\n",
      "进行到input_size= 687\n",
      "进行到input_size= 688\n",
      "进行到input_size= 689\n",
      "进行到input_size= 690\n",
      "进行到input_size= 691\n",
      "进行到input_size= 692\n",
      "进行到input_size= 693\n",
      "进行到input_size= 694\n",
      "进行到input_size= 695\n",
      "进行到input_size= 696\n",
      "进行到input_size= 697\n",
      "进行到input_size= 698\n",
      "进行到input_size= 699\n",
      "进行到input_size= 700\n",
      "进行到input_size= 701\n",
      "进行到input_size= 702\n",
      "进行到input_size= 703\n",
      "进行到input_size= 704\n",
      "进行到input_size= 705\n",
      "进行到input_size= 706\n",
      "进行到input_size= 707\n",
      "进行到input_size= 708\n",
      "进行到input_size= 709\n",
      "进行到input_size= 710\n",
      "进行到input_size= 711\n",
      "进行到input_size= 712\n",
      "进行到input_size= 713\n",
      "进行到input_size= 714\n",
      "进行到input_size= 715\n",
      "进行到input_size= 716\n",
      "进行到input_size= 717\n",
      "进行到input_size= 718\n",
      "进行到input_size= 719\n",
      "进行到input_size= 720\n",
      "进行到input_size= 721\n",
      "进行到input_size= 722\n",
      "进行到input_size= 723\n",
      "进行到input_size= 724\n",
      "进行到input_size= 725\n",
      "进行到input_size= 726\n",
      "进行到input_size= 727\n",
      "进行到input_size= 728\n",
      "进行到input_size= 729\n",
      "进行到input_size= 730\n",
      "进行到input_size= 731\n",
      "进行到input_size= 732\n",
      "进行到input_size= 733\n",
      "进行到input_size= 734\n",
      "进行到input_size= 735\n",
      "进行到input_size= 736\n",
      "进行到input_size= 737\n",
      "进行到input_size= 738\n",
      "进行到input_size= 739\n",
      "进行到input_size= 740\n",
      "进行到input_size= 741\n",
      "进行到input_size= 742\n",
      "进行到input_size= 743\n",
      "进行到input_size= 744\n",
      "进行到input_size= 745\n",
      "进行到input_size= 746\n",
      "进行到input_size= 747\n",
      "进行到input_size= 748\n",
      "进行到input_size= 749\n",
      "进行到input_size= 750\n",
      "进行到input_size= 751\n",
      "进行到input_size= 752\n",
      "进行到input_size= 753\n",
      "进行到input_size= 754\n",
      "进行到input_size= 755\n",
      "进行到input_size= 756\n",
      "进行到input_size= 757\n",
      "进行到input_size= 758\n",
      "进行到input_size= 759\n",
      "进行到input_size= 760\n",
      "进行到input_size= 761\n",
      "进行到input_size= 762\n",
      "进行到input_size= 763\n",
      "进行到input_size= 764\n",
      "进行到input_size= 765\n",
      "进行到input_size= 766\n",
      "进行到input_size= 767\n",
      "进行到input_size= 768\n",
      "进行到input_size= 769\n",
      "进行到input_size= 770\n",
      "进行到input_size= 771\n",
      "进行到input_size= 772\n",
      "进行到input_size= 773\n",
      "进行到input_size= 774\n",
      "进行到input_size= 775\n",
      "进行到input_size= 776\n",
      "进行到input_size= 777\n",
      "进行到input_size= 778\n",
      "进行到input_size= 779\n",
      "进行到input_size= 780\n",
      "进行到input_size= 781\n",
      "进行到input_size= 782\n",
      "进行到input_size= 783\n",
      "进行到input_size= 784\n",
      "进行到input_size= 785\n",
      "进行到input_size= 786\n",
      "进行到input_size= 787\n",
      "进行到input_size= 788\n",
      "进行到input_size= 789\n",
      "进行到input_size= 790\n",
      "进行到input_size= 791\n",
      "进行到input_size= 792\n",
      "进行到input_size= 793\n",
      "进行到input_size= 794\n",
      "进行到input_size= 795\n",
      "进行到input_size= 796\n",
      "进行到input_size= 797\n",
      "进行到input_size= 798\n",
      "进行到input_size= 799\n",
      "进行到input_size= 800\n",
      "进行到input_size= 801\n",
      "进行到input_size= 802\n",
      "进行到input_size= 803\n",
      "进行到input_size= 804\n",
      "进行到input_size= 805\n",
      "进行到input_size= 806\n",
      "进行到input_size= 807\n",
      "进行到input_size= 808\n",
      "进行到input_size= 809\n",
      "进行到input_size= 810\n",
      "进行到input_size= 811\n",
      "进行到input_size= 812\n",
      "进行到input_size= 813\n",
      "进行到input_size= 814\n",
      "进行到input_size= 815\n",
      "进行到input_size= 816\n",
      "进行到input_size= 817\n",
      "进行到input_size= 818\n",
      "进行到input_size= 819\n",
      "进行到input_size= 820\n",
      "进行到input_size= 821\n",
      "进行到input_size= 822\n",
      "进行到input_size= 823\n",
      "进行到input_size= 824\n",
      "进行到input_size= 825\n",
      "进行到input_size= 826\n",
      "进行到input_size= 827\n",
      "进行到input_size= 828\n",
      "进行到input_size= 829\n",
      "进行到input_size= 830\n",
      "进行到input_size= 831\n",
      "进行到input_size= 832\n",
      "进行到input_size= 833\n",
      "进行到input_size= 834\n",
      "进行到input_size= 835\n",
      "进行到input_size= 836\n",
      "进行到input_size= 837\n",
      "进行到input_size= 838\n",
      "进行到input_size= 839\n",
      "进行到input_size= 840\n",
      "进行到input_size= 841\n",
      "进行到input_size= 842\n",
      "进行到input_size= 843\n",
      "进行到input_size= 844\n",
      "进行到input_size= 845\n",
      "进行到input_size= 846\n",
      "进行到input_size= 847\n",
      "进行到input_size= 848\n",
      "进行到input_size= 849\n",
      "进行到input_size= 850\n",
      "进行到input_size= 851\n",
      "进行到input_size= 852\n",
      "进行到input_size= 853\n",
      "进行到input_size= 854\n",
      "进行到input_size= 855\n",
      "进行到input_size= 856\n",
      "进行到input_size= 857\n",
      "进行到input_size= 858\n",
      "进行到input_size= 859\n",
      "进行到input_size= 860\n",
      "进行到input_size= 861\n",
      "进行到input_size= 862\n",
      "进行到input_size= 863\n",
      "进行到input_size= 864\n",
      "进行到input_size= 865\n",
      "进行到input_size= 866\n",
      "进行到input_size= 867\n",
      "进行到input_size= 868\n",
      "进行到input_size= 869\n",
      "进行到input_size= 870\n",
      "进行到input_size= 871\n",
      "进行到input_size= 872\n",
      "进行到input_size= 873\n",
      "进行到input_size= 874\n",
      "进行到input_size= 875\n",
      "进行到input_size= 876\n",
      "进行到input_size= 877\n",
      "进行到input_size= 878\n",
      "进行到input_size= 879\n",
      "进行到input_size= 880\n",
      "进行到input_size= 881\n",
      "进行到input_size= 882\n",
      "进行到input_size= 883\n",
      "进行到input_size= 884\n",
      "进行到input_size= 885\n",
      "进行到input_size= 886\n",
      "进行到input_size= 887\n",
      "进行到input_size= 888\n",
      "进行到input_size= 889\n",
      "进行到input_size= 890\n",
      "进行到input_size= 891\n",
      "进行到input_size= 892\n",
      "进行到input_size= 893\n",
      "进行到input_size= 894\n",
      "进行到input_size= 895\n",
      "进行到input_size= 896\n",
      "进行到input_size= 897\n",
      "进行到input_size= 898\n",
      "进行到input_size= 899\n",
      "进行到input_size= 900\n",
      "进行到input_size= 901\n",
      "进行到input_size= 902\n",
      "进行到input_size= 903\n",
      "进行到input_size= 904\n",
      "进行到input_size= 905\n",
      "进行到input_size= 906\n",
      "进行到input_size= 907\n",
      "进行到input_size= 908\n",
      "进行到input_size= 909\n",
      "进行到input_size= 910\n",
      "进行到input_size= 911\n",
      "进行到input_size= 912\n",
      "进行到input_size= 913\n",
      "进行到input_size= 914\n",
      "进行到input_size= 915\n",
      "进行到input_size= 916\n",
      "进行到input_size= 917\n",
      "进行到input_size= 918\n",
      "进行到input_size= 919\n",
      "进行到input_size= 920\n",
      "进行到input_size= 921\n",
      "进行到input_size= 922\n",
      "进行到input_size= 923\n",
      "进行到input_size= 924\n",
      "进行到input_size= 925\n",
      "进行到input_size= 926\n",
      "进行到input_size= 927\n",
      "进行到input_size= 928\n",
      "进行到input_size= 929\n",
      "进行到input_size= 930\n",
      "进行到input_size= 931\n",
      "进行到input_size= 932\n",
      "进行到input_size= 933\n",
      "进行到input_size= 934\n",
      "进行到input_size= 935\n",
      "进行到input_size= 936\n",
      "进行到input_size= 937\n",
      "进行到input_size= 938\n",
      "进行到input_size= 939\n",
      "进行到input_size= 940\n",
      "进行到input_size= 941\n",
      "进行到input_size= 942\n",
      "进行到input_size= 943\n",
      "进行到input_size= 944\n",
      "进行到input_size= 945\n",
      "进行到input_size= 946\n",
      "进行到input_size= 947\n",
      "进行到input_size= 948\n",
      "进行到input_size= 949\n",
      "进行到input_size= 950\n",
      "进行到input_size= 951\n",
      "进行到input_size= 952\n",
      "进行到input_size= 953\n",
      "进行到input_size= 954\n",
      "进行到input_size= 955\n",
      "进行到input_size= 956\n",
      "进行到input_size= 957\n",
      "进行到input_size= 958\n",
      "进行到input_size= 959\n",
      "进行到input_size= 960\n",
      "进行到input_size= 961\n",
      "进行到input_size= 962\n",
      "进行到input_size= 963\n",
      "进行到input_size= 964\n",
      "进行到input_size= 965\n",
      "进行到input_size= 966\n",
      "进行到input_size= 967\n",
      "进行到input_size= 968\n",
      "进行到input_size= 969\n",
      "进行到input_size= 970\n",
      "进行到input_size= 971\n",
      "进行到input_size= 972\n",
      "进行到input_size= 973\n",
      "进行到input_size= 974\n",
      "进行到input_size= 975\n",
      "进行到input_size= 976\n",
      "进行到input_size= 977\n",
      "进行到input_size= 978\n",
      "进行到input_size= 979\n",
      "进行到input_size= 980\n",
      "进行到input_size= 981\n",
      "进行到input_size= 982\n",
      "进行到input_size= 983\n",
      "进行到input_size= 984\n",
      "进行到input_size= 985\n",
      "进行到input_size= 986\n",
      "进行到input_size= 987\n",
      "进行到input_size= 988\n",
      "进行到input_size= 989\n",
      "进行到input_size= 990\n",
      "进行到input_size= 991\n",
      "进行到input_size= 992\n",
      "进行到input_size= 993\n",
      "进行到input_size= 994\n",
      "进行到input_size= 995\n",
      "进行到input_size= 996\n",
      "进行到input_size= 997\n",
      "进行到input_size= 998\n",
      "进行到input_size= 999\n",
      "进行到input_size= 1000\n",
      "进行到input_size= 1001\n",
      "进行到input_size= 1002\n",
      "进行到input_size= 1003\n",
      "进行到input_size= 1004\n",
      "进行到input_size= 1005\n",
      "进行到input_size= 1006\n",
      "进行到input_size= 1007\n",
      "进行到input_size= 1008\n",
      "进行到input_size= 1009\n",
      "进行到input_size= 1010\n",
      "进行到input_size= 1011\n",
      "进行到input_size= 1012\n",
      "进行到input_size= 1013\n",
      "进行到input_size= 1014\n",
      "进行到input_size= 1015\n",
      "进行到input_size= 1016\n",
      "进行到input_size= 1017\n",
      "进行到input_size= 1018\n",
      "进行到input_size= 1019\n",
      "进行到input_size= 1020\n",
      "进行到input_size= 1021\n",
      "进行到input_size= 1022\n",
      "进行到input_size= 1023\n",
      "进行到input_size= 1024\n",
      "进行到input_size= 1025\n",
      "进行到input_size= 1026\n",
      "进行到input_size= 1027\n",
      "进行到input_size= 1028\n",
      "进行到input_size= 1029\n",
      "进行到input_size= 1030\n",
      "进行到input_size= 1031\n",
      "进行到input_size= 1032\n",
      "进行到input_size= 1033\n",
      "进行到input_size= 1034\n",
      "进行到input_size= 1035\n",
      "进行到input_size= 1036\n",
      "进行到input_size= 1037\n",
      "进行到input_size= 1038\n",
      "进行到input_size= 1039\n",
      "进行到input_size= 1040\n",
      "进行到input_size= 1041\n",
      "进行到input_size= 1042\n",
      "进行到input_size= 1043\n",
      "进行到input_size= 1044\n",
      "进行到input_size= 1045\n",
      "进行到input_size= 1046\n",
      "进行到input_size= 1047\n",
      "进行到input_size= 1048\n",
      "进行到input_size= 1049\n",
      "进行到input_size= 1050\n",
      "进行到input_size= 1051\n",
      "进行到input_size= 1052\n",
      "进行到input_size= 1053\n",
      "进行到input_size= 1054\n",
      "进行到input_size= 1055\n",
      "进行到input_size= 1056\n",
      "进行到input_size= 1057\n",
      "进行到input_size= 1058\n",
      "进行到input_size= 1059\n",
      "进行到input_size= 1060\n",
      "进行到input_size= 1061\n",
      "进行到input_size= 1062\n",
      "进行到input_size= 1063\n",
      "进行到input_size= 1064\n",
      "进行到input_size= 1065\n",
      "进行到input_size= 1066\n",
      "进行到input_size= 1067\n",
      "进行到input_size= 1068\n",
      "进行到input_size= 1069\n",
      "进行到input_size= 1070\n",
      "进行到input_size= 1071\n",
      "进行到input_size= 1072\n",
      "进行到input_size= 1073\n",
      "进行到input_size= 1074\n",
      "进行到input_size= 1075\n",
      "进行到input_size= 1076\n",
      "进行到input_size= 1077\n",
      "进行到input_size= 1078\n",
      "进行到input_size= 1079\n",
      "进行到input_size= 1080\n",
      "进行到input_size= 1081\n",
      "进行到input_size= 1082\n",
      "进行到input_size= 1083\n",
      "进行到input_size= 1084\n",
      "进行到input_size= 1085\n",
      "进行到input_size= 1086\n",
      "进行到input_size= 1087\n",
      "进行到input_size= 1088\n",
      "进行到input_size= 1089\n",
      "进行到input_size= 1090\n",
      "进行到input_size= 1091\n",
      "进行到input_size= 1092\n",
      "进行到input_size= 1093\n",
      "进行到input_size= 1094\n",
      "进行到input_size= 1095\n",
      "进行到input_size= 1096\n",
      "进行到input_size= 1097\n",
      "进行到input_size= 1098\n",
      "进行到input_size= 1099\n",
      "进行到input_size= 1100\n",
      "进行到input_size= 1101\n",
      "进行到input_size= 1102\n",
      "进行到input_size= 1103\n",
      "进行到input_size= 1104\n",
      "进行到input_size= 1105\n",
      "进行到input_size= 1106\n",
      "进行到input_size= 1107\n",
      "进行到input_size= 1108\n",
      "进行到input_size= 1109\n",
      "进行到input_size= 1110\n",
      "进行到input_size= 1111\n",
      "进行到input_size= 1112\n",
      "进行到input_size= 1113\n",
      "进行到input_size= 1114\n",
      "进行到input_size= 1115\n",
      "进行到input_size= 1116\n",
      "进行到input_size= 1117\n",
      "进行到input_size= 1118\n",
      "进行到input_size= 1119\n",
      "进行到input_size= 1120\n",
      "进行到input_size= 1121\n",
      "进行到input_size= 1122\n",
      "进行到input_size= 1123\n",
      "进行到input_size= 1124\n",
      "进行到input_size= 1125\n",
      "进行到input_size= 1126\n",
      "进行到input_size= 1127\n",
      "进行到input_size= 1128\n",
      "进行到input_size= 1129\n",
      "进行到input_size= 1130\n",
      "进行到input_size= 1131\n",
      "进行到input_size= 1132\n",
      "进行到input_size= 1133\n",
      "进行到input_size= 1134\n",
      "进行到input_size= 1135\n",
      "进行到input_size= 1136\n",
      "进行到input_size= 1137\n",
      "进行到input_size= 1138\n",
      "进行到input_size= 1139\n",
      "进行到input_size= 1140\n",
      "进行到input_size= 1141\n",
      "进行到input_size= 1142\n",
      "进行到input_size= 1143\n",
      "进行到input_size= 1144\n",
      "进行到input_size= 1145\n",
      "进行到input_size= 1146\n",
      "进行到input_size= 1147\n",
      "进行到input_size= 1148\n",
      "进行到input_size= 1149\n",
      "进行到input_size= 1150\n",
      "进行到input_size= 1151\n",
      "进行到input_size= 1152\n",
      "进行到input_size= 1153\n",
      "进行到input_size= 1154\n",
      "进行到input_size= 1155\n",
      "进行到input_size= 1156\n",
      "进行到input_size= 1157\n",
      "进行到input_size= 1158\n",
      "进行到input_size= 1159\n",
      "进行到input_size= 1160\n",
      "进行到input_size= 1161\n",
      "进行到input_size= 1162\n",
      "进行到input_size= 1163\n",
      "进行到input_size= 1164\n",
      "进行到input_size= 1165\n",
      "进行到input_size= 1166\n",
      "进行到input_size= 1167\n",
      "进行到input_size= 1168\n",
      "进行到input_size= 1169\n",
      "进行到input_size= 1170\n",
      "进行到input_size= 1171\n",
      "进行到input_size= 1172\n",
      "进行到input_size= 1173\n",
      "进行到input_size= 1174\n",
      "进行到input_size= 1175\n",
      "进行到input_size= 1176\n",
      "进行到input_size= 1177\n",
      "进行到input_size= 1178\n",
      "进行到input_size= 1179\n",
      "进行到input_size= 1180\n",
      "进行到input_size= 1181\n",
      "进行到input_size= 1182\n",
      "进行到input_size= 1183\n",
      "进行到input_size= 1184\n",
      "进行到input_size= 1185\n",
      "进行到input_size= 1186\n",
      "进行到input_size= 1187\n",
      "进行到input_size= 1188\n",
      "进行到input_size= 1189\n",
      "进行到input_size= 1190\n",
      "进行到input_size= 1191\n",
      "进行到input_size= 1192\n",
      "进行到input_size= 1193\n",
      "进行到input_size= 1194\n",
      "进行到input_size= 1195\n",
      "进行到input_size= 1196\n",
      "进行到input_size= 1197\n",
      "进行到input_size= 1198\n",
      "进行到input_size= 1199\n",
      "进行到input_size= 1200\n",
      "进行到input_size= 1201\n",
      "进行到input_size= 1202\n",
      "进行到input_size= 1203\n",
      "进行到input_size= 1204\n",
      "进行到input_size= 1205\n",
      "进行到input_size= 1206\n",
      "进行到input_size= 1207\n",
      "进行到input_size= 1208\n",
      "进行到input_size= 1209\n",
      "进行到input_size= 1210\n",
      "进行到input_size= 1211\n",
      "进行到input_size= 1212\n",
      "进行到input_size= 1213\n",
      "进行到input_size= 1214\n",
      "进行到input_size= 1215\n",
      "进行到input_size= 1216\n",
      "进行到input_size= 1217\n",
      "进行到input_size= 1218\n",
      "进行到input_size= 1219\n",
      "进行到input_size= 1220\n",
      "进行到input_size= 1221\n",
      "进行到input_size= 1222\n",
      "进行到input_size= 1223\n",
      "进行到input_size= 1224\n",
      "进行到input_size= 1225\n",
      "进行到input_size= 1226\n",
      "进行到input_size= 1227\n",
      "进行到input_size= 1228\n",
      "进行到input_size= 1229\n",
      "进行到input_size= 1230\n",
      "进行到input_size= 1231\n",
      "进行到input_size= 1232\n",
      "进行到input_size= 1233\n",
      "进行到input_size= 1234\n",
      "进行到input_size= 1235\n",
      "进行到input_size= 1236\n",
      "进行到input_size= 1237\n",
      "进行到input_size= 1238\n",
      "进行到input_size= 1239\n",
      "进行到input_size= 1240\n",
      "进行到input_size= 1241\n",
      "进行到input_size= 1242\n",
      "进行到input_size= 1243\n",
      "进行到input_size= 1244\n",
      "进行到input_size= 1245\n",
      "进行到input_size= 1246\n",
      "进行到input_size= 1247\n",
      "进行到input_size= 1248\n",
      "进行到input_size= 1249\n",
      "进行到input_size= 1250\n",
      "进行到input_size= 1251\n",
      "进行到input_size= 1252\n",
      "进行到input_size= 1253\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2a91486df40>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA/xUlEQVR4nO3dd3hUxfrA8e9kd9N7IYSEEpAiIL2KBUUEFQVsgN17FQvea++9XvvPil4LiAUBBb3YUYogoBCK9J4AIb3X3WyZ3x9nSSEJCZBG9v08zz57ds6cc2Y2m3dn58yZo7TWCCGE8AxezV0AIYQQTUeCvhBCeBAJ+kII4UEk6AshhAeRoC+EEB7E3NwFqEtkZKTu1KlTcxdDCCFOKuvWrcvSWkcdmd7ig36nTp1ISEho7mIIIcRJRSm1v6Z06d4RQggPIkFfCCE8iAR9IYTwIBL0hRDCg0jQF0IIDyJBXwghPIgEfSGE8CAS9IUQzW7VoVUcKDjQ3MXwCC3+4iwhROumteaW327BR0PCBXMguldzF6lVk5a+EKJZZVuzAbApYOWbzVsYDyBBXwjRrHKsORXL3n7NWBLPIEFfCNGsrA5r+bLNJ7AZS+IZ6gz6Sqn2SqmlSqntSqmtSqk73enhSqlflVK73c9hlbZ5WCm1Rym1Uyk1plL6QKXUZve6t5RSqnGqJYQ4WdictvJl+1/vw8fnN2NpWr/6tPQdwL1a61OBYcA0pVRP4CFgsda6K7DY/Rr3uslAL2AsMF0pZXLv6z1gKtDV/RjbgHURQpyEKrf0r20XzWf525qxNK1fnUFfa52qtV7vXi4EtgOxwHhgljvbLGCCe3k8MEdrbdNaJwJ7gCFKqRggWGu9WmutgU8rbSOE8FBWZ0XQzzGZeDki7Ci5xYk6pj59pVQnoD/wFxCttU4F44sBaOPOFgscrLRZsjst1r18ZHpNx5mqlEpQSiVkZmYeSxGFECcZq720uYvgUeod9JVSgcB84C6tdcHRstaQpo+SXj1R6w+01oO01oOioqrd+EUI0YqUVhq9c9j+/SvY9sO/wVEGG7+EooxmKFnrVK+Ls5RSFoyA/4XWeoE7OV0pFaO1TnV33Rz+qyQD7SttHgekuNPjakgXQniqpJU8u+61asnjlt0OwIT/9sTkcnL7d/m0ebz6l0O5jO2w43s46/7GKmmrUZ/ROwr4GNiutX690qqFwPXu5euB/1VKn6yU8lFKxWOcsF3j7gIqVEoNc+/zukrbCCE8UMrnFx91/beBfswPDmRUh1jQNXYMGD4eA0ueA+kqqlN9undGANcC5yqlNrofFwIvAqOVUruB0e7XaK23AvOAbcDPwDSttdO9r9uAjzBO7u4FfmrIygghTi5L/P0BuD03r868+bt+rHXdDl3CDm8LOMtqzfPT+vf47sfbjrmMrU2d3Tta6z+ouT8eYFQt2zwPPF9DegLQ+1gKKIRovb4JDKC7rYyLi4qZHhZ61LwF2TsJ4aJq6VprroiNIcLhZJnTXuv2D2yeDsA5JZkE+jfSuUKnHX55FEbcCSE1jlNpdnJFrhCiWWit2eftzemlVmIdTkYWl9DG4aiW79ygLgAUFqbBtoV89u01XPZBdz7/bz8+mnU2pYWHAMg2m8Bhq7b9kdZvmd2wFaks8XdY81/46YHGO8YJkqAvhGgWBWUFOBREhXdDPZrG29et5puBj1bLd06vq438RSkkLLyJl/P/ZpePNy/5OnmTHNJmjKnIfJTunRBtdFhs3/drw1akssNfOqV5sK1lnrKUoC+EaBbZpcbsmpFmP7D4QWAbgvtdw7TcPC4sKubttEw6Kl/6th0EwKqM9dwYE11tP+PDKoWx2oL+9u9xuU8tlriP29C01jy3Zx5T2kWTmPInzLsOMnc1yrFOhAR9IUSzyCrNAiDCUnWStVu7XMZLmdmMLC3l+7GfER8STzQmvvGpe59XrLiXtOK0ioTkdexJ38BnP91KqXuqr5Kywgarw2HWpJW8/35v5mavZ4uPD5fEteO90GDY8nWDH+tESdAXQjSLrFLjavtI34iqK8b9HzyeBU/lQ5seAIRbgsgzmapkuy6/gHFFxVXSdhTu58EFl0LeQePCro/O5e4fb+DliDAc7qBfai+GP/6vQevyzqqnme5fNZx+EBqC/feXGvQ4DUGCvhCiWWSlrAMgIvyUqiuUApOlSlJ4YAwAQ0utvDDkUf6Ku4L7J8zDJ6Z/tf2udxWyas5EHMWZfBfgT3hZSZX1JV5e8NtTsGkeWmv00cb/11OqxSjvoNKKeYQcSrHdx/vo1xc0Awn6QohmkZ29E4vWBMefU2fe+GgjuHdu05eLT52M/6gnoMNQLgjtSR+rjQuOaPHvchTxybZZPNImkvW+voQ7nbyYkUVPm41fA/x5PzSY0zY8S59P+/Di7w9CQQokrji+ijhsFDpKCHc6eSfd+PVyeBTSe6EhUFZ0fPttJBL0hRDNIqM0iwinRsX0qTPvFT0mMTxmOFee82KV9KGnXsEXqem8POZDbu5yKaf5G78I/AOj+Tt1TXm+TnY7FxWX4BtgnAh+t9I1AbP3/4R+/VSYNe74KvJcGxJL0hhRYiVg8M38dfqr/HzavYxQASRaLGDNr5rf5ax5P01Egr4QollsKcuiB5a6MwKdQzrzwfkfcErYEV1BMX3giRw4ZRT/PuNpPprwLQBFvsEkl1Sc0B1otcHjWXRoP6LG/Z/Xvh2zgoOMi6uOQW5hCtfHtCHNbCbeOxQufAX/rmOwDLmJtuFdKVNUvXYgaw88Ew4/PgB2K7zWA6affkzHPFES9IUQTU4XZXHIZSPeHHTiO/OqOMHrZ/bDrCEzYxNJjopulfYdR4LJwgPDHiXIyxuAG/MK8HO5AMgwm/k2KACsR5tAuLpFuxaw3tcXgHhzQJV1Pl4+2JSqMoy05MBq1vr6ULL2A177ZDgTgzVrCvZU2a7YXkxRI3YJ1WuWTSGEaCi2lI0Uzzwfe1w0UfbaL6Y6HkopQpxOVvv54lCKV8KGkt7lTMb1uAqAIO8gVpz/KX9+ci5DSq30sdm4O9qYkmGPtzf2n+7H4hMEF79Zr+PluoedAnQwVR166mP2pUypipa+y8lDW95nafm1Bg7Am3/GRLPZnVJYVsioeedS6rSy/rIlWAIbfroIaekLIZpMnjWPQb9ey9lxRuCLPaJ13BCyzSb2ehut+SGnXsH1va7HUmk0kMk/ghGlVixAXI8JVbb9R/YfJG76vO4RN7t/g7TNZJVm4uNy8Vp6Jt32r6mSxdtktPS13Uq+LZ/TPuvH0tpuReJysilxMad/eTql7juJHVzxYs15T5AEfSFE08jZx769P5e/DHU6GRbavVEPGR7SsXpiYDQMux1uWUHAsGlVVm309eG6mGiwl1Tfzs3uKOOBRVPZNuMc5qYsR6M4v6QULv2gSj4fsy9aKXKs2WzJ2lLr/rrZysCaz9XL76qS/nLWakodDT9VtHTvCCEaX/ZeeHsAf4UGQ1god+fkcnVBIT5Xv9Lgh3r5rJd5YLl7wjPfkOoZlIKx/wHAz90946U1LvfFW3kmE3r1dNTZNd+QZXfir/wUGMBPgcavlDIvBdfMh1POq5LP22z09Y/88yE6+VRcgHZfdi4ZZhNrfX0J8A1jn9nJc8sfKl/fzu4gxWJmpasQh6v6BHQnSlr6QoiGU5AKpbk1pB8i0WIunz75xvxCfOKGgU9g9bwnaEynShOw1RT0Kwn1CWWAXwxvd76ySnry9m+Mhaw9Rp0AnA743zR+3/xp9R21rT7s1NfsV76cZMtmeGkp3x1M4bprl3C/OZZ5KWkMCuxIjsnE3PTV5Xkfz85hnFcYD9ssBHk3wInuI0hLXwjRcF7vYQTahw4YrxNmoJNWckfRZpbHtQPg7JJS1EMHwN0SbmheqlJb1uJXe0bA7GVm1pWLALhs00zmBxlfQukdhxr3fH1noJHxqXzI3A4bPuer9u3AXBE6v0hJA/8jppIABkX2xcflwuZllGdYqZVODgdEdQN3t01UUCwUbgPgvOIS/p2bR7zdwRnhg2HL/OOpfp2kpS+EaFjW/IoLkL6/m9Qd37BcGUHOS2veTM80vhjM9ZhB7Ti1cTiItTuMrpx6eiIrxwjgQL79iCGTTjvphck8ERlOptnMHbl5TCooxNfl4lRbWZVho4d1iTyVVfuTGV1snB+Is1fqqpnwHsSfRWSlaSRuCe5F/NTVcMsKCGxr/GKqx/0BjpW09IUQx2xP7h78UjYSe8oY8HF3QbicvBQeyio/P/77fBQZZhOnAWmmijAzubAY0zULGr18v4x8B3WMU954PZBIZGkWfH8ZBY5iWP9ZxcpnIzkvvgO4fwnE2R3cPPotHvjqBixnP1TzDn1D8QZuycvH3HUMZ3fuAoNvMtZ1GAbXf0dU6try7JHdLzZ+BQAoLwir4SR0A5CgL4Q4ZhMXTgRg2F/Pc9uo1xnQ8Rx2vt2Hz0ODARjdwbhVYH+rlQ3ui5cuD+3N7ZPfB5+j97M3BHOXGu/kenT+4YR7+2PWmqSDf+DY9AO3tY3iT7/qXUSRTidevSbi3Wti7fvzNk70di+z8/Lo6TVmiQpuX74cMeimihVtexuPRiBBXwhx3P40Odj/2zQW/XMbs00lQNUTs4cDPsDdXS4nuAkC/onwNfvS12pjtZ8f+V6mGgM+wABrPbpdDnctRZ1aa5YIv4pzAeoYuqJOhAR9IUTtnO5+8Rr6rA9LNZtIfD6C1Igw2jgcZJhrDivBEafUmN7SDLHaeC8sxJgWGQi1BJFnN2688kJgby6+6L9gq+d0DXdvq+j+qoHFy8IFRcWMLGn48fi1kRO5Qoja/ScO3hlcZ7ZL4tqx2s+P/rW0gGelpENM34YuXaMY0f+mKq+XTPq9fDnUEgC+wRASV7+dhcQa+Y/i5cxsLiyu/WKwhiZBXwhRO0cp5OytknS0m45Mih5eLW1d4gEGXPx+gxetsfQdUXFi9sH4iVWmcAhV3s1RpAYlQV8IUSOXdnFvVATrfKoOrbQ6rTXmvzc7l8EXvcucvvfyfKcJ5eneY1+E3pc2ZlEbltmbWGXU+ZqhxpW9IV7G6xizf8Mfr9/V0Hlkw++3FtKnL4SoosCay/T5lzNpwB0sCgxgUWBA+SyQYEz9C8atAbvY7dwy8G7Slj3HaWVl4BdKr3430MuaT2nCDGIcDhhVd/dQSzN30mKyS7PLrxie3nEiO9ZOJ7JXz4Y/2ISaR/Y0Fgn6QogqPlv3Nl84MshYeh8EVG/Z5lqNaRYmFxQypqQUBt1EVJ+roKjipiX4hjDp7gOQuROiGyFQNrIQnxBCKo006jPsbvpE9oYeFzdjqRqGBH0hRBW6NAeAXysF/KzSLCL9ImHnzyxK+g6AMJfL6JbwDjAeR8797mU6KQN+jbz9oef45i5Fg5A+fSFEFY4aphU+WHgQh8tByZzJLEheAkCH/jfCdf9r6uKJEyRBXwhRRaY1p1qa7eBfPLbiUYZ2ao8TxWUFRbQd+XgzlE6cKOneEUJUkVVW/cKjsoQZ/OBrXECUbTYR7B3UKNMii8YnLX0hRIXMXRwoy8d8xFh8W7uqF1YVuK9QFScfCfpCeJrc/bDh8+rpf8+lZPoQkl0lnOe+X7mPyRifnusepnnYVbammSdGNDzp3hHC08y9BtI2QZdREByDztrD319Npm9OCru8jatPL8jLInbUQwyIHsC0xdPId1Q9uRt7wevNUXLRACToC+FhDhanEODlRXjmdpK1jQt+mAj+cH+pFzNDjGGXfaw2zh14Fznuk7oFR1yF69/jkiYvt2gYEvSFaMm0Pqa7P9XF4XJwYaQ/RPqzviSHn7evKV/3SkQYAL1sNiLPMuaf8TUZUyNnF6WANwz2joDwzigv6Rk+WclfToiWav2n8HQoFGU22C6zCpLLl+9f8SBv7vyiWp6HI4bByAcB8Lf409FuJ8HLDsDVQT2YMWZGg5VHND0J+kK0UM61H7HU3495Wz5psH2mbZlTvrzYfcXto1k5XFlQiJfyYsWVy+l72adVtokw+ZPrbtn71HGjcdHySfeOEC1J+lbwC0cHteU60tgUHQU7P+WKofc1yJ2V0rTRYj/VVoafdjE1r4DTS63osE7cO3kV/u5b/FVW6huEtczYzsciY/NPdnW29JVSM5RSGUqpLZXSnlJKHVJKbXQ/Lqy07mGl1B6l1E6l1JhK6QOVUpvd695STXVvMCFOJu+dDq/3YP+HZ7LJt2JK4+Tt32J1WDlt1mn8d/G9VbfJ3gtPhUDyujp3n+Y+Mfth/3uZlZrBiFIr6ur5eN35d40BH2B7WcUVut5+4cdRKdGS1Kd75xNgbA3p/6e17ud+/AiglOoJTAZ6ubeZrpQ6fJ+194CpQFf3o6Z9CuG5tMYFvBgexnOu9CqrLlz7BO/MvQiAd5IXcdmckVjz9gOQuXAac4ICWffZBThcjqMeYkvGeiIcToK7XQSPZcIjqdD1vHoXMTAgqu5MokWrM+hrrZcD1SfjqNl4YI7W2qa1TgT2AEOUUjFAsNZ6tTZuu/MpMOE4yyxEq7Q7czN94zvwRUgQf/kZo2bmJ6eWr5/lyChf3mXLZvD/xlH68WjuKdvH85Hh3NAumve/mQy2ohr373Q5WWzP5vRSK8onEMzexuyRxyA8qJ63CRQt1omcyL1DKbXJ3f0T5k6LBQ5WypPsTot1Lx+ZXiOl1FSlVIJSKiEzs+FGLgjRks3Y/FGV10prOkyZf9RtbrcnsbFSN9Ce9A2w/GXjRVkxuJzl63Jz9+FQit42G3gfX998SHD749pOtBzHG/TfA7oA/YBU4DV3ek399Poo6TXSWn+gtR6ktR4UFSU/J4Vn6OBttJ1uysvnkawcNiYdxDemD/f0vxOAJ7KyedRedfRMgvsXwWEr/XxJKTgILhe80A7nZxPRy1/Flbufgx+cDkBEUKwx1309dfKu6Mf38o84rrqJluO4gr7WOl1r7dRau4APgSHuVclA5aZAHJDiTo+rIV0I4VZizcXX5eJOHcKUwiK8rlsIfmHc2OcmEpIOcEVhMRdOnF1tuzFFxTydmc0LmVlYvbz4PWkRc/6vPfssZp4s3EyfxFnc9+W5XNeuLQDdcw4dU7lmTVhQ8cI3pPaM4qRwXEM2lVIxWuvDnY0TgcMjexYCs5VSrwPtME7YrtFaO5VShUqpYcBfwHXA2ydWdCFal69TluOvNUz6AmwF0OmM8nU+09aBfzjB/uHc3PM6+ntH0itpDQEbZ+MT0hGKsrGPepxH9s3ihUijZR7gclHsHl9/+C5Y09My6GQOPaZyhftF8ELnKwnI3d+gVweL5lFn0FdKfQmMBCKVUsnAk8BIpVQ/jC6aJOAWAK31VqXUPGAb4ACmaa0PdyrehjESyA/4yf0QQhSmg8UPjaatwwHRveHIaQ4iTylf/Pfg+40Fh4INs6HTmTBhExaAfbMAGFVcUn7xFcApdgcvxI7lVJ8MGPXkMRfx4jPlhimtRZ1BX2s9pYbkj4+S/3ng+RrSE4Dex1Q6IVq5xPxEUt8fxkBLJMURXpxrjqwe8GvjE2Q8V7pK9pM25zJ733e8NPB+vvvjWfK9vLiu7614jXqsEUovTkZyRa4QTcTpcvLauteY3H0yHXzCoCiDK3+4DGvbNryTlgG0IeJYRtWcOh5GPgzDbitPGnjeSwxcHQf9r2FiZFdY/hqceU/DV0actJTWtQ6iaREGDRqkExISmrsYQpywXbm7uGzhZZxWZmdGShq+WnNafIcqed5yRnDOP5Y1TwFFq6KUWqe1HnRkurT0hWgihUVpAGz2tjC4U83j3QNbdhtMtAIyy6YQTWT/6jdqTP8sJY3bcvMBaC/tMNHIJOgL0UTWF1cfH78x8QD97trN7SOeYE3SQdoqCfqicUnQFw0jJ9F4iFrtp4z+ViuXFBpz40wyRWK6fCb4hUKfyfhFdIPRzzRvIUWrJ0H/ZFOYDofWN97+ty2E1L/rzud0QEkOJK4wyvNWP3j/jKNv47AZUwBvrH5VaYujNfz8CBxcU3feuqRtgYNryXBaibM7eT4rh82JB3jsmqXQ+1Ijj08g3LEG2g85+r6EOEHyW/Jk8+E5UHAInsg5pvlT6rTnN+yr3mF16moGWW34d78I1+UzyLXmEvHbM7DxC3g8C0wWAGw/3oNe/ym+7tFfLsCrrObZHcuVZBvPi5+Bflc1XNmPV2keWPyN2SaPVJgKf74Lm7+C+3dDWQmYfeseQ28rgtIcCK00Kuf9EQDkdYwjpNIEaEI0B2npn0ySE9hqzWCRvx9k7qg7//5V8EIs5BrzrmMrrPl+q1l74PPL+CZzLdPatmFs+3aU7PyBr1c8zcj5o/lj5/yK7QG05sq0RYyLi2Gfxcxp8R3oG9+BV8NDK/b59xzY9FXV4zjLjGevFtLWeKkjfHV9jav01m/Z6OONDmkHvzwKL8TAj/fVvc9Px8Mbp1XdFzAxti0lXl6EulzG2Pq2p9W8vRCNTIJ+U9Ia3hpgBMRj5XKRNXM0k2NjuDc6imJ36/GoZl4AZUXw/V3G6+mnw6unVM2z9mNypw9mcMc4nnXP2ZJrMnFb2yj+3volAK+Eh3HAbKbkkwvh0wmU/fke+7wtpJvNjI9rV76rWSHBFVP5fnMLLLip6rHspcZzQ/5COV6Hr0/Z+aPx5XhYWQkseZ7v9//Mte3a8l3Bblj9jrFu3Uzj2emo2P5Ih6peU7It8Tf6xHdgj7fxayJuxP0w8iG49Y+GrI0Q9SZBvynZSyBnrxEQj9Xqt3kvtGKGw0eiIowWaGW1BaPDafkHqqYXZ8EP9/CHvy9Wd7fFqbYy2tkdrPf1ZWGQcXXoPm8LF7Vvx9CAYu4p2syCVdVm2aCf1QbAWx/2h5IcfvH3I8Nkqgj0YMzvDi2jpX/4VwcY5zGA5IKD7P9uGix/mT+zNgPwaFQEeeVdOsZkY/qN3vDZxDr2b9xTdubvj5QnjQ/rzQWD7miY8gtxnCToN6XD3SPH4ZukX5gXHMTgUiudtYklAf6s3PdzRYbCNHg2AtbPqntn7tZ4Sdom7o+K4JGoSHxcLmakpvNRWjoz0ypu1XezqnpP1F8D/Hk+smra9xd+yZN24/6qH/pqTp9zJvdFR3F3m0j49Ul4to3xxXM46KsW0NKv/GVkMr6ELp1/AeOKErglOqr8Cw/guQj3PYK0kwXLHmd8MOQk/Q57Fhsnpg/VcG9aWyF2l53VLuM8R3dbGc+0H4eXkn850bxa9SewqDiDl6Z350DCh81dFEPl29gVZdSe70jLXuIJh3FDsiFWKx/3M+ZSudfXSsnnlwPgSt/KTwH+FG6aC+s+gQN/VmyvnfDt7RWvrfmgNd8ue5SfA41g/a/cfAZf/yvBQ6fRzhzIC5lZtLfbuWnU6/za81/lmw62GS3Y9nY7my75nk2TVtExqjendL+EHw8a49ALTcbHKstkYtnmT7k1Mpgf511GYc4elvr7obN2wsYv61//xuCwViyvepv0tI2Uehkt+VX+xgRmL2cXck1+AYsD/Fnh58tp8R14cv+3JHpb+CEwgLVfT2FmSBB8eC7sXVr+i8oJ8HI8qdu/Jd9k4tnMbL5OScOrJXRrCY/XqoP+j9u+5PMAbx5d94oR6BpLykaYe43R2p5/M3x+WY3dLHZrLgsCAyhUCl7tWnNXzOHW465FADgL03hiy/vlq2/MLyCy92RGOM0Ue3kx1LkTngrhjR/+wQNtIpmTtwW+uxNmjKnYZ+Jyftm1gC+C3a1XWyHsXcKukjRCnU7mHUrl2oJCiOkLY56HB/dzcVEJPyan4t9uAG2D2/NYVg7zk1P5+ObtvJKRxYdpGSi/MJSve6ZHpWjvcHJfdi4APlqRbjbxr7ZRrPT340Hrbk7f+gb/jo5ija8PLHuhQd7641W49kO+CA7kychw5gYFMnvuJZi15u20TAZYrTyQncsF4b2Jd2gcSnF72zZVtn85Iox/xETzengYeywW+GwCfHoJy/z8GNypPd8EBpCyzOgGi3W4b1YurXzRArSAztXGk5i3F4DNPt7Y3z8Dy12bG/4gT1X0sy/P3cFvjmymFBRyallRxdS3br8f+oMnoyLY5uPNY9m5RmvTUvX2d2x2j3iZfQUHxr/JtZvfJsfd1RDkdOET0BbM3lw/+B5WrjfuhbowMICZocEAvBUeypWFRfi5XHwTFMgSf7/ylivAAKuNU8uKwWFjn7eZznY7p96bBC57RRmUArMfDJ1qLHcbyyT3BUUoxdjL5kDCjCPqZ7SSryeY6xMP8NvQ67g7Y1mNb1mSxcJQS0A93tzGc/rBeRBhdFMtcFdjUKmVkSqAkWM+gd9fgilziEr+HVY8UL7dsv3JzA8K5O1KI5UWBgZQ6KVIK9nOlqhw7ErxbGQ4dmV0o7W3S9AXLUerDfqHcvfyecpSAJxK8ZUzhwYfGe5yVXk5zc8GBJLg68OPK98yhuZVGte9+tBKAOYGB9GlzM6UsuLqQb+sGDvwp58vvye8QY53xbjuB3NywWzco3T4adfygdPO1L//j0ejqt639IyOcRwpxuEgzWTimchwPvv5YUzDbmevxcLY4hLjwqAjPZZWsXzk3ZI6n208Kgt2j+I5816w+HNenyv54KspLE1fw7S8PLwvnYH31zdyWWxb3gwL5YyUXcQ67cZJ3Sa+G9OuzIov/0sLi/jDz5cMs5kxxSUw7W8IiIROxsndAbGn08tmo6PdwYuZ2ajBNzN16K2EfXoOC2K74+dlZuYR+z+7pJTf3V+0Jq2Jvvhd2L4Quo1tqioKUatW2fRwaRdjF04A4KKiYsKdTrYFnvi9PatNQ203ArQTeObwyT7goMXC4P2zse8yTrTmpG3iXzMHMq9wZ3meFyLDje6gX580hgmW77OE5yPCub1tG+a6A36o08mmxAOMLyqGi98qzzq8w0geycopf730QDITCqteIDW01Mq/cvJYdDCFZ+PGsMXHh18y1vB12h8UmEx0ttupl65joN2A2tcP+gdc9jEMvBH6TQEvE8MnzeORnFxCXBq/3pdiOudRnszKodDkxdj2sdiei+Sb9/uRtXle/cpwokrzoCiDed/fDMDMjFyezsph7qE03kvLYNLpjxkBv5IQnxDmpKTzUmY2atoauOhViDyFK+45yJeTfuPec17DUmm+nKm5+Txz3SpCnU6628r4ITkF1W8KTPmy5i9XIZpYq51P/+dF97Bpx3zuzsnjhlN64Veax0c3bzvucpQ6Srli3mj6R/bm2fONPvaczG2c98OV2Cu1VB8tLOP5IGNM9qcZ+RwYPpXH9s4tX/9MZjZPuFvmoVoxOS+PaXn58JRxzuGBT4byk6r4Eng4K4crCouw3L/PuHL0iC4jngrhlfBQVvn58s0ho3X+c4A/KWYT3crsnFFqhYAoGPkw9rB4Ji67HTuKHG9frNrBj73+TftBNx/3+1Knw91f7vrpWZfwSNEWvg+s6N4ZYLUy65bdxjmOrN3gHQAFKdB+cMOVw+WEZ8LJ8/LizI5xBDldrGp3CYx5AZ4OhR7jYPIXNW97+IK2wKgaV2eXZmPathDv7+/G77rvUPFnYn8qxLh94YNJ4BdW43ZCNCaPm09/rCmUsTl5MOB6osoSSSrNNkbMBLY5+oaOMmOaA5cDe1hHPlxyPwM6jOSJPx4h1Wxmf+pKns5Nwiu0I3ctnlYl4AMM941hVdJGzuoYxxJvReHmz8Df+BJYkJxKV7uduHOe4B9b3iVPad4PC6GD3c7FZcUsTni3POC/mp6Jj4azSkvxOuMeCIioVlQAHsvg/s8vg6QVcMnbEBTD2KC2sPQFOPViI5j5Gv39ltS/mVJQyIsR4aAdzDuUSvsrLz+h97lO9+2uMkRTXTWX577+JwcKEtjk6wPAel9f1q54gcHKH36rdP/Wp+p58j3vgDG/Tdxg2P8HfHWDkf7PX425bJwOvp41krc7xJJjMsryXFY2nHOB0bV0357y96hGtQT7wyL8IoxfOANvLE+zXPQ67PheAr5ocVpt0CdnH4S0h0veIvKnm1hcnMShN3sS+3DG0edPmXlB+VWVfw29kfcylkDKEjBXvFXPfT6SSW3PYIMti05ldi4pKmaHt4WHs3OJHDAakjfQzuHgy+BAQlwuutvK+Dqloo98cNyZ3PDHf/gsJAinUnwSEsyo/8Ryl/vGGg9n5TCmxD2O/MJXYchRWuJmH6NlDOAfAV1HG8tTahgSGdOXK/vdyg+7P6NbmZ1Tgzs3flA68kvW4ocprCOf7PyBrT7exNsdjGnfjlv3zeFKK1xtNhHnOGJ+moztENmt1it51844m4/9TKz096ONw8HwyHDGFRUzbOG/yQ2MwDvyVJ425QPG9j4uF+cOvRfiz3KX8ehB/bgM/qfxEKKFaZV9+gCkb4WOxlQFXiajRXlVTBvI3H7UzWwpCeR7KUqU4raMxVXWvZ5u/Mz/KjiIdzJXA/BmRiY3T/mR11zhRLpc0GkEXPUVbUI6YfPyIsNsZlJhpYuybvwZQuK4NzePjUkHmZabxy4fb4a6A/5zmdlcNexBI9gDRHWvu659JxvPbfvUmdVy7uPMTk3nqewcaNOj7n03hrMfwBIcSz9bGSEuF7fl5lOG5nNfzYTYGDb6uCdAc7kgPxmmD4MX2sG868FeaXx93gHsB9dwU3gAK90nTjPMZv4XFMhd0VH8x5nG2Xo/w7J+AeAa/3heychiXkqaMTxVCA/Uelv6t/9ZfgWsy33FZY7JxNzPz2fSXfshaydE96qySdrK1xndqUO1XQHMPpTGaWVlnF9UzKLAALb6eDO01ErniJ7G5FlBbSF7N4R2griBvKlcXL78bgaXWrl8zLvw9Q3GjmIHGn3z4/4PHDam+IXy7kZjzLpZa8YVFRut2m5joMNwaNu77rr2mmg86uuRFPj2Nhj9bP23aUh+YXDPNqPfftM8Ri97hlfdJ8ItwLXt2gLw3/f70z7nIO9FRvBYdg72HQv5dUU72q2ZwUCbFW8Ne7wtuGJjABhdXMKvAf4AFHt5MTuk6vmPf174EZGzJxtX0IZ1bLr6CtGCtNoTuZXlJa/l2f9NYlFgAMNLS/kgPRu0C25eCrHuESkH13Lakn9U2/Y9Zxhn9J9qTAsc1omkBf/g4vbG8MR25kB+uewXoz84fStsmgujnjK6j7Q2ThCC0TedusmY3GvkQ1UPkLGdpA/O4MmocK7PL+TcktL692W3Fp9NJGX/77RxONl+4Qtctd04UT4lv5CdPhbW+/rWuYtvk1PoYnew2teHYJdmcqzxxfFlkZkNtkyinE7G3p9qnCj++0s457G6p0kW4iTmcSdyKwsN7cRrmdnc6uXFSn8/9pm96Gx3QXJCedDPmnk+1DC+fUTc2dD/GuPF5q/p5HDwn4wsHm4TybX9p1WcAIzuVfWuR0eOPY/pYzyO5BdOJ4eDWakZcPV8iKv2N2r9Js+m3fNGkD6tx6Vs/vERbo2O4ssjWupH02Hcu1Cay/C4IVCcwUu/3MUuXUrPi2bSe+3H0OUcI2NkVxj1RGPUQoiTgkcEfQKj4MEk4j7qD8ANMdE8nJ3LWT8/QMDQqeC0Mz4upjz7fdm5vBoRxp05eairH6vYT2RXAMZ1HM1F3S5E9Zx89OPevKRiquHa+LsnLxv9LHQ975ir1ipY/IxAHBQDwTEw5BbO3PFFeT/94+0vYsWOr0g3mxlXVMxVBYWc0TGOYndLvbfNhqVv1b/FhcCF6Vugy7lwyqimrpEQLZZHdO8clj/7St7NWlPegpxUUMhjQx5hb7veTFhiTHf8WUoafW1lrPX1ob/2wfLwwao7KUyDwOgmv4rU02QdWMkdP9/I8FIrd955oGK8v/IC7SLZbCLApck1eRHhdBHyRG7zFliIFqa27h2P6tQM0ZqJRRVXrM4NDmLGqmfYlGSM0plzKI1+A6aigCGmYCy3ray+k6C2EvCbQKRPKHNS0rnTN95IiD8LzrgHHtgHQJzDSdhF/0fnIdMIeSyrGUsqxMnFo1r67PwZ/eUknokII9tkYmmAP/FldgbYbPwU4M/q1Dy87t8LlrpPHIpG5nLBosdgyE0Q3rnqupVvQmR36C5z2QhRm9pa+p4V9A9zdxVcHd+NTRjjvi8oKublG9bWfuWrEEKcRKR7pwbbqbjQZ2RJqQR8IUSr55lB/8af4ZJ3eFgbI2fCnE5jWl0hhGjlPGPI5pE6DoeOw7kidiBXLHoMOg6FMwc2d6mEEKLReWbQPyy6J1y7oLlLIYQQTcYzu3eEEMJDSdAXQggPIkFfCCE8iAR9IYTwIBL0hRDCg0jQF0IID1Jn0FdKzVBKZSiltlRKC1dK/aqU2u1+Dqu07mGl1B6l1E6l1JhK6QOVUpvd695SSmYtE0KIplaflv4nwJEzWz0ELNZadwUWu1+jlOoJTAZ6ubeZrpQ6fDfr94CpQFf3Q2bLEkKIJlZn0NdaLwdyjkgeD8xyL88CJlRKn6O1tmmtE4E9wBClVAwQrLVerY0Z3j6ttI0QQogmcrx9+tFa61QA93Mbd3osUPmuI8nutFj38pHpNVJKTVVKJSilEjIzM4+ziEIIIY7U0Cdya+qn10dJr5HW+gOt9SCt9aCoqKgGK5wQQni64w366e4uG9zPGe70ZKB9pXxxQIo7Pa6GdCGEEE3oeIP+QuB69/L1wP8qpU9WSvkopeIxTtiucXcBFSqlhrlH7VxXaRshhBBNpM5ZNpVSXwIjgUilVDLwJPAiME8p9U/gAHAFgNZ6q1JqHrANcADTtNZO965uwxgJ5Af85H4IIYRoQp55u0QhhGjl5HaJQgghJOgLIYQnkaAvhBAeRIK+EEJ4EAn6QgjhQSToCyGEB5GgL4QQHkSCvhBCeBAJ+kII4UEk6AshhAeRoC+EEB5Egr4QQngQCfpCCOFBJOgLIYQHkaAvhBAeRIK+EEJ4EAn6QgjhQSToCyGEB5GgL4QQHkSCvhBCeBAJ+kII4UEk6AshhAeRoC+EEB5Egr4QQngQCfpCCOFBJOgLIYQHkaAvhBAeRIK+EEJ4EAn6QgjhQSToCyGEB5GgL4QQHkSCvhBCeBAJ+kII4UEk6AshhAeRoC+EEB5Egr4QQngQCfpCCOFBJOgLIYQHkaAvhBAe5ISCvlIqSSm1WSm1USmV4E4LV0r9qpTa7X4Oq5T/YaXUHqXUTqXUmBMtvBBCiGPTEC39c7TW/bTWg9yvHwIWa627Aovdr1FK9QQmA72AscB0pZSpAY4vhBCinhqje2c8MMu9PAuYUCl9jtbaprVOBPYAQxrh+EIIIWpxokFfA4uUUuuUUlPdadFa61QA93Mbd3oscLDStsnutGqUUlOVUglKqYTMzMwTLKIQQojDzCe4/QitdYpSqg3wq1Jqx1HyqhrSdE0ZtdYfAB8ADBo0qMY8Qgghjt0JtfS11inu5wzgG4zumnSlVAyA+znDnT0ZaF9p8zgg5USOL4QQ4tgcd9BXSgUopYIOLwPnA1uAhcD17mzXA/9zLy8EJiulfJRS8UBXYM3xHl8IIcSxO5HunWjgG6XU4f3M1lr/rJRaC8xTSv0TOABcAaC13qqUmgdsAxzANK2184RKL4QQ4pgcd9DXWu8D+taQng2MqmWb54Hnj/eYQgghToxckSuEEB5Egr4QQngQCfpCCOFBJOgLIYQHkaAvhBAeRIK+EEJ4EAn6QgjhQSToCyGEB5GgL4QQHkSCvhBCeBAJ+kII4UEk6AshhAeRoC+EEB5Egr4QQngQCfpCCOFBJOgLIYQHkaAvhBAeRIK+EEJ4EAn6QgjhQSToCyGEB5GgL4QQHkSCvhBCeBAJ+kII4UEk6AshhAeRoC+EEB5Egr4QQngQCfpCCOFBJOgLIYQHkaAvhBAeRIK+EEJ4EAn6QgjhQSToCyGEB5GgL4QQHkSCfhNzujRa6+YuRrNwuTR3zdnAhgO5zV0UALTWLN+VicvlmX8P4Zkk6Dchl0vT5ZEfef6H7c1dlGaRXVzGtxtTuPnThOYuCgC/bkvnuhlrmLkqqbmLItySsop54cft8kXciCToNyGbwwXAR38kNnNJmofL/Qunpfw/p+ZbAdifXdzMJTk5FdkcWO3OBt3nbV+s54Pl+9ibWXTc+9iZVsj21IIGLFXr0mqDfmmZk8xCW4Pu0+nSvLNkN4VW+zFvm1Fo5actqSd8/NV7s09oHwdzShqle2lzcj6dHvqBHWm1/7PZ7MaXnrOFRH2ljOfG7m2zOZx893dKg7/vB3NK6PTQD6zYndmg+62PxdvT6f3kL/R4/OcG3W+Zw/gSOZGPyJg3lnPBmysaqETV2RxOCo4jBrQUrTbon//G7zz7/bby16v2ZJW3StYk5jDylaUU2RzHtM+rPvyTVxft4uWfd5anLdqaxoHsEoDyn6QZhdYq290zbyNDnl/MPfP+Lk97e/FudqYVVjvG/uxiLnnnDw5kl/D6op3Yna7yde8s2cOUD/9kTWJOvcq7N7OIm2atJa+kDICfNqdy5stLmbkyqX4VrkVqfinfbjhUJe3we718V+0ByFb+D62rvPf5JfYqLbu/9mWz5VB+lVZkfqmd5NwSzn11GY9/uwWAGX8k8u8vNxx3PZJzS6u8Xrozg1+2plXL99u2dDIKrNXSa/L9phTunruxStorP+/kX19uYPXe7GqBv6ZujEKrnQXrk6ukrU3KITm3pEraeve5kWs/XsOcNQfqVb7D2z35vy3VyqK1JiEppzw9v9Re6xfV3LUHq6UV2RzYHE7KHC5+35XJT5tTj/n8jXJ/E1f+3J+I5bsymfbFerTW7EgrYG1SDv/9fS8Hc0pwuI+RXmDl3aV7+LIe76HWmonvrqLPU4vK06x2J498s5nrZqwhMaviV2NSVjGdHvqBO+cYn9G8kjLS6/k5akzm5i5AY+nWJohVe7PYkVZAoI+Zqz76i0v6tuPu0d248r+rAdiZVsDAjuGA8cdUSuFwutiaUkDbEF+ig30BKLY5WH8gl7/cwTYxqxir3YlLa6Z+tg6AG07vxCerkph/23Aue281T4zryT/OiMdqd7Jg/aFq5Xvt11289usutj49Bg0EeJtwaZjywZ+k5Fs565WlABRYHTxy4al4m73YmpIPQFaR8Qtm1Z4s1iTlsDWlgF+3pfP+NQMoKHUwJD6cYD8Lo177HYBps9ezKTmfQqsRaOclHOSG0zsxY2Uikwa3x9/bzIw/EhnQMRSzlxef/bmfW87qTNsQX4J8LXR66AdMXordz12Al5fils/WsSk5n7O6RZGSV8ovW9NYk2S8N4u2pjP1rC4UWO3cOHMtE/q149rhnSgtc5Z3bxVaHfR+8he+unU4gzqGMeXDP9mWWsCbk/txdrcoJn3wp/E3jA7kpzvPwkvBhW+u4FCeEaT3ZRVjcziZl2AExu5tg7h9ZBd+2ZrOyO5R+FpM1d7vMoeL5NwSdqYVkltiZ/Lg9nywfJ/xt0djtTu5ceZaAL6+dTgDO4YBUGp3cpP7HMTcqcMY2jkCgOwiGyF+Fswmo920dGcGfhYTd8w2/sEvHRDL/uwSRveMLu/OS84rZfh/ljD1rM5MGtyey99fzfbUAv55RjyPj+vJ2qQc+saFcsfsDfy+K5P4yABiQ/34al0yr/yyE7OXYu2j5/H95lR6xgRVCSAPLdjMhX1iCPIxM3/9IbzNXlzStx3vLt3D6V0i+GN3Fmd1iyIqyIdLp68CIMjXwjtL9/DbPWdzSptAvkpI5oH5m3j9yr5c1CeGvk8vYlSPNtw9uhvpBVb2ZhYx9awuxv9EWdUGk9aa3k/+QlSQD13bBLKq0i/STU+dT7CvpdrfJC3fyl1zN+BtNvHQ2B7sySwq72p7ddFOlu3MZFjncO47vzt2p6bM6eLsblFV9jF92R5e/WUnKx48ly2H8qv87bXWXDdjDQCPF/Rk7BsVrf///LSDc7pHMeKUSJ6rdI5typAObDiQS6CPmbxSOwHeZmwOJ/07GJ+Hy99fzbZKXUdFNgd3z93Ir9vSATjn1WUAfDttBBPeXQnA/zamcG6PNjyyYDPFZU6SXryofPtCq52krBIyCq38c5bxOdvw+Gg2H8pn8fZ07hvTnaAa3rsToZp6JIlSaizwJmACPtJav3i0/IMGDdIJCcd+4u/5H7bx4Yqj952f26MNVw/twHvL9pKwP5ebz4zH5nDx6er9AMy/bTir92bz6qJd1bY9q1vUUVu1AB9eN4gAHxNXffhXvcrsZzFRWksf6bZnxnDH7A0s2ZHBm5P74dKau+f+XWPe+jB5KZwuzeie0Vzct12NLeaIAG/uHt2Nx9wt67gwPw7lldImyIf0gtq7zpbff075l1ZDePnyPjzw9aZ65e3fIZRnx/dm9d5srju9I//9fR85xWV8csTJ2jcm9eOuI1rk9bH16TG8u3QP05ftxeylWHjHGfznp+2s2J1VY/5ze7RhyY6MOvf7811nVglKtTm7WxS/1/G5O+z9awZw6+fr65X3hYmn8cg3m+vM9/aU/sRHBvDwgs1sPpRfnlZsc/DQgpq37xYdyPf/OhNvsxenPfkLNqeLtyb359bP19WrbJVdNbQD953fnZV7slixO7P8i78m/zr3FGauTKLI5iA21K+80XA0tX0u3r9mIGuTcvi40vm4X+46izFvLK9xP8M7R7B6X81dsQ9f0IMft6SxJ72Q4rKa/99P7xLBuv25bH16THnD4lgppdZprQdVS2/KoK+UMgG7gNFAMrAWmKK13lbbNscb9Kcv21OlG0Z4HrOXwtFCzh8IcawiA71JeGz0cW9fW9Bv6j79IcAerfU+rXUZMAcY3xgHig7ybYzdNorIQG9G94xu7mK0OvUN+G2DfXnsolPxtTTsv0N8ZAAAFpNq0P0e6dL+sY26/+Yy9azO9c7br31o4xWkmQT6NE7ve1P36ccClc8AJQNDj8yklJoKTAXo0KHDcR1ofL92dIsOosjmICEph3N6tCE5t5SZKxN5fFxP8kvtmL0Uy3dnEh8ZyKCOYXy9LpmSMieFVjvj+8Xywo/befWKvhRY7bzx2y5uHBFPfGQA89YeLO+Hv2xgHDNXJnHjiE4U25yEBVgoc7hIzbcyfdledqcX8sG1g8gvtePSmhA/C38n59EpIoBSu5M1iTlM7B9Lj7ZB5SeBCqx2/tyXg5/FRN/2IXSODOTT1fvJKbYRHxlAu1A/eseGsHpvNgE+ZtqG+FJa5qTI5uBATglOl4siq4NgPwtf/HXA6MftH0tsmB8jukTyd3Ie+7NLWL47k0v7x/F3ch692gWX97tvPJhHYlYx7149gIJSOz9uTqVnTDBOrekUEcB/ftrOHed0ZenODPJKyhhxSiQJSbncce4pLNyYwumnRBAX5s/X65IptNrpEO7PwI5h7MsqJrPQRpi/NxaTYtXebC4dEMuO1EI2JeczuJORJzGrmKwiG0Piw+kWHcTri3Zx/ekdOe/UaB5esJkzukZid2rC/C18ucb4OD04tjtrk3IJD7DgazER4GPm0W82Exnow1OX9KJjhD/z1x3iz33ZrD+Qi8OpmXnjYE6NCS7/zNx0ZmdsDie704tIzi3lnB5RrNyTRXiAD/syi4z3OtiXYD8LK/dkYXe62J9dQscIf7pEBdIh3J+oIB/+Sswmr8TOmV2jKLDaiQnx5ZsNh4gL8ycy0Ju/9uVw2cA41h/IpUO4P9lFZTy8YBPn9YzG5dKcFhdKdJAPJXYnGw7kEeRjZmNyHou2pnHr2V0Y368dK3ZnsSk5n96xIfxjRCfuG9Odj1Yk4qUgtcBKz5hgtNbERwby1bqDTOwfi7fJC2+zFzvSCpkypAPz1yUztndbfCxevLNkD1pDz3bBdIzwZ1daIRsP5uFjNnFhnxhC/Sz0iAkiv9SO06VZk5jDz1vSsDlcXDoglgM5JXSJCiSj0MYFvdsSGeiD06VJzCrC22QiItCb37anU2xzMqF/O/wsJmwOF+v259IpMoBZq5KICPDm/F5tScwqIjrYl17tQrhqSAfW7c8lPMCb5LxSdqcXYjF50b9DKL5mE7klZYzvF4u32Yuc4jKCfM3YHC5yi8tYvTebr9Yd5K0p/TmUW4rTpckpLqN3bAjZxWWsd++3zOGiW9sg2gb7MmNlIn4WEwM6hrE7vZCk7GKeGNeLhxZswtvkhdaQXmilQ7g/U8/qjJ/FxA+bUxndM5rsojJ2ZxRSZHWQWVTGNcM60CbIl8SsYhxOFzaHi/nrk+nRNoi4MH9OaRNIeIA36QVW5qw5SEpeKQ+M7UFJmYMtKQX0jQs5rthXl6bu3rkCGKO1vsn9+lpgiNb6X7Vtc7zdO0II4claSvdOMtC+0us4IKWJyyCEEB6rqYP+WqCrUipeKeUNTAYWNnEZhBDCYzVpn77W2qGUugP4BWPI5gyt9damLIMQQniyJr84S2v9I/BjUx9XCCFEK56GQQghRHUS9IUQwoNI0BdCCA8iQV8IITxIk0+4dqyUUpnA/uPcPBKoeSask0trqEdrqANIPVoaqUftOmqto45MbPFB/0QopRJquiLtZNMa6tEa6gBSj5ZG6nHspHtHCCE8iAR9IYTwIK096H/Q3AVoIK2hHq2hDiD1aGmkHseoVffpCyGEqKq1t/SFEEJUIkFfCCE8SKsM+kqpsUqpnUqpPUqph5q7PEejlGqvlFqqlNqulNqqlLrTnR6ulPpVKbXb/RxWaZuH3XXbqZQa03ylr0opZVJKbVBKfe9+fTLWIVQp9bVSaof7bzL8JK3H3e7P0xal1JdKKd+ToR5KqRlKqQyl1JZKacdcbqXUQKXUZve6t5RSjXvPyvrV4xX352qTUuobpVRos9RDa92qHhhTNu8FOgPewN9Az+Yu11HKGwMMcC8HYdw4vifwMvCQO/0h4CX3ck93nXyAeHddTc1dD3fZ7gFmA9+7X5+MdZgF3ORe9gZCT7Z6YNyWNBHwc7+eB9xwMtQDOAsYAGyplHbM5QbWAMMBBfwEXNAC6nE+YHYvv9Rc9WiNLf0mu/l6Q9Bap2qt17uXC4HtGP+04zECEO7nCe7l8cAcrbVNa50I7MGoc7NSSsUBFwEfVUo+2eoQjPHP+jGA1rpMa53HSVYPNzPgp5QyA/4Yd6hr8fXQWi8Hco5IPqZyK6VigGCt9WptRM5PK23TJGqqh9Z6kdba4X75J8adA6GJ69Eag35NN1+PbaayHBOlVCegP/AXEK21TgXjiwFo487WUuv3BvAA4KqUdrLVoTOQCcx0d1N9pJQK4CSrh9b6EPAqcABIBfK11os4yepRybGWO9a9fGR6S/IPjJY7NHE9WmPQr6nPq8WPS1VKBQLzgbu01gVHy1pDWrPWTyk1DsjQWq+r7yY1pLWEv5EZ4yf5e1rr/kAxRndCbVpkPdx93uMxugraAQFKqWuOtkkNac1ej3qordwtuj5KqUcBB/DF4aQasjVaPVpj0D/pbr6ulLJgBPwvtNYL3Mnp7p93uJ8z3OktsX4jgEuUUkkY3WnnKqU+5+SqAxjlStZa/+V+/TXGl8DJVo/zgEStdabW2g4sAE7n5KvHYcda7mQquk4qpzc7pdT1wDjganeXDTRxPVpj0D+pbr7uPhv/MbBda/16pVULgevdy9cD/6uUPlkp5aOUige6YpzsaTZa64e11nFa604Y7/cSrfU1nER1ANBapwEHlVLd3UmjgG2cZPXA6NYZppTyd3++RmGcKzrZ6nHYMZXb3QVUqJQa5q7/dZW2aTZKqbHAg8AlWuuSSquath5NeUa7qR7AhRijYPYCjzZ3eeoo6xkYP9k2ARvdjwuBCGAxsNv9HF5pm0fdddtJE49KqEd9RlIxeuekqwPQD0hw/z2+BcJO0no8DewAtgCfYYwMafH1AL7EOA9hx2jp/vN4yg0Mctd9L/AO7tkHmrkeezD67g//n7/fHPWQaRiEEMKDtMbuHSGEELWQoC+EEB5Egr4QQngQCfpCCOFBJOgLIYQHkaAvhBAeRIK+EEJ4kP8HVSwgvH6ORswAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 预测代码\n",
    "losses = []\n",
    "predictions = []\n",
    "actuals = []\n",
    "for i in range(start_input, input_size + 1):\n",
    "    print(\"进行到input_size=\", i)\n",
    "    # gru=GRUModel(i, hidden_size, output_size, layers_size).to(device)\n",
    "    gru = GRUModel(30, hidden_size, output_size, layers_size).to(device)\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(gru.parameters(), lr)\n",
    "\n",
    "    # 数据，以比特币为例\n",
    "    trainB_x = torch.from_numpy(ji[i - 30:i].reshape(-1, batch_size, 30)).to(torch.float32).to(device)\n",
    "    trainB_y = torch.from_numpy(ji[i].reshape(-1, batch_size, output_size)).to(torch.float32).to(device)\n",
    "\n",
    "    loss = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        output = gru(trainB_x).to(device)\n",
    "        loss = criterion(output, trainB_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(\"loss\"+str(epoch)+\":\", loss.item())\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # 预测，以比特币为例\n",
    "    pred_x_train = torch.from_numpy(ji[i - 29:i + 1].reshape(-1, 1, 30)).to(torch.float32).to(device)\n",
    "    pred_y_train = gru(pred_x_train).to(device)\n",
    "    # print(\"prediction:\",pred_y_train.item())\n",
    "    # print(\"actual:\",Data.iloc[i+1,1])\n",
    "    predictions.append(pred_y_train.item())\n",
    "    actuals.append(ji[i + 1])\n",
    "plt.plot(losses)\n",
    "\n",
    "plt.plot(predictions)\n",
    "plt.plot(actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1224\n"
     ]
    }
   ],
   "source": [
    "print(np.array(predictions).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1224\n"
     ]
    }
   ],
   "source": [
    "print(np.array(actuals).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1224\n"
     ]
    }
   ],
   "source": [
    "print(input_size-29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f=open('2 问题1/周期lstm黄金预测.csv','w',encoding='utf-8',newline=\"\")\n",
    "csv_writer=csv.writer(f)\n",
    "csv_writer.writerow([\"实际价格\",\"预测价格\"])\n",
    "for i in range(0,input_size-29):\n",
    "    tmp=[]\n",
    "    tmp.append(actuals[i])\n",
    "    tmp.append(round(predictions[i],2))\n",
    "    csv_writer.writerow(tmp)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**回归**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn import linear_model\n",
    "# from d2l import torch as d2l\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import xlwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rcz\\AppData\\Local\\Temp/ipykernel_21304/4261364655.py:1: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  BCHAIN_MKPRU=pd.read_csv(r\"C:\\Users\\rcz\\Desktop\\运筹学\\代码及数据\\1 数据预处理\\原始数据\\BCHAIN-MKPRU.csv\",dtype={\"Date\":np.str,\"Value\":np.float64})\n",
      "C:\\Users\\rcz\\AppData\\Local\\Temp/ipykernel_21304/4261364655.py:2: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  LBMA_GOLD=pd.read_csv(r\"C:\\Users\\rcz\\Desktop\\运筹学\\代码及数据\\1 数据预处理\\原始数据\\LBMA-GOLD.csv\",dtype={\"Date\":np.str,\"Value\":np.float64})\n"
     ]
    }
   ],
   "source": [
    "BCHAIN_MKPRU=pd.read_csv(r\"C:\\Users\\rcz\\Desktop\\运筹学\\代码及数据\\1 数据预处理\\原始数据\\BCHAIN-MKPRU.csv\",dtype={\"Date\":np.str,\"Value\":np.float64})\n",
    "LBMA_GOLD=pd.read_csv(r\"C:\\Users\\rcz\\Desktop\\运筹学\\代码及数据\\1 数据预处理\\原始数据\\LBMA-GOLD.csv\",dtype={\"Date\":np.str,\"Value\":np.float64})\n",
    "Data=pd.read_csv(r\"C:\\Users\\rcz\\Desktop\\运筹学\\代码及数据\\1 数据预处理\\处理后数据\\C题处理后的中间文件.csv\")\n",
    "df = Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def to_timestamp(date):\n",
    "    return int(time.mktime(time.strptime(date,\"%m/%d/%y\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#将日期变为自然数\n",
    "start_timestamp=to_timestamp(Data.iloc[0,0])\n",
    "for i in range(Data.shape[0]):\n",
    "    Data.iloc[i,0]=(to_timestamp(Data.iloc[i,0])-start_timestamp)/86400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "days_fit=Data.shape[0] # 最小为2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bFit=Data.iloc[0:days_fit,0:2]\n",
    "gFit=Data.iloc[0:days_fit,0::3].dropna() # 需要考虑NaN的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bitcoin_reg=linear_model.LinearRegression()\n",
    "gold_reg=linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bitcoin_reg.fit(np.array(bFit.iloc[:,0]).reshape(-1,1),np.array(bFit.iloc[:,1]).reshape(-1,1))\n",
    "gold_reg.fit(np.array(gFit.iloc[:,0]).reshape(-1,1),np.array(gFit.iloc[:,1]).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitcoin: [[29568.99959448]]\n"
     ]
    }
   ],
   "source": [
    "print(\"bitcoin:\",bitcoin_reg.predict(np.array([days_fit]).reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold: [[1843.56496063]]\n"
     ]
    }
   ],
   "source": [
    "print(\"gold:\",gold_reg.predict(np.array([days_fit]).reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 预测代码：\n",
    "b_pred_linear = [None, None]\n",
    "g_pred_linear = [None, None]\n",
    "for day_fit in range(2, days_fit + 1):\n",
    "    # print(\"进行到day_fit=\",day_fit)\n",
    "    bFit = Data.iloc[0:day_fit, 0:2]\n",
    "    gFit = Data.iloc[0:day_fit, 0::3].dropna()\n",
    "\n",
    "    bitcoin_reg = linear_model.LinearRegression()\n",
    "    gold_reg = linear_model.LinearRegression()\n",
    "\n",
    "    bitcoin_reg.fit(np.array(bFit.iloc[:, 0]).reshape(-1, 1), np.array(bFit.iloc[:, 1]).reshape(-1, 1))\n",
    "    gold_reg.fit(np.array(gFit.iloc[:, 0]).reshape(-1, 1), np.array(gFit.iloc[:, 1]).reshape(-1, 1))\n",
    "\n",
    "    b_pred_linear.append(bitcoin_reg.predict(np.array([day_fit]).reshape(-1, 1)))\n",
    "    g_pred_linear.append(gold_reg.predict(np.array([day_fit]).reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rcz\\AppData\\Local\\Temp/ipykernel_21304/3966830884.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ji1=np.array(b_pred_linear).reshape(-1,1)\n"
     ]
    }
   ],
   "source": [
    "ji1=np.array(b_pred_linear).reshape(-1,1)\n",
    "ji1=np.array(ji1)\n",
    "ji2=Data.iloc[2:days_fit+1,1]\n",
    "ji2=np.array(ji2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ji3=[]\n",
    "\n",
    "for i in range(2,1826):\n",
    "    ji3.append(round(ji1[i][0][0][0],2))\n",
    "\n",
    "ji3=np.array(ji3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "603.3499999999999\n"
     ]
    }
   ],
   "source": [
    "print(ji1[3][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "book = xlwt.Workbook(encoding=\"utf-8\", style_compression=0)\n",
    "sheet = book.add_sheet(\"回归预测比特币\", cell_overwrite_ok=True)\n",
    "col = (\"日期\",\"预测值\",\"真实值\",\"误差\")  # 元组,如果需要院校简介另加\n",
    "for i in range(0, 4):\n",
    "    sheet.write(0, i, col[i])\n",
    "sheet.write(1,0,\"9/11/16\")\n",
    "sheet.write(1,1,621.25)\n",
    "sheet.write(1,2,621.25)\n",
    "sheet.write(1,3,0)\n",
    "\n",
    "sheet.write(2,0,\"9/12/16\")\n",
    "sheet.write(2,1,609.67)\n",
    "sheet.write(2,2,609.67)\n",
    "sheet.write(2,3,0)\n",
    "for i in range(0, 1824):\n",
    "    print(\"第%d条\" % (i + 1))\n",
    "    sheet.write(i+3,0,df.values[i+2][0])\n",
    "    sheet.write(i+3,1,ji3[i])\n",
    "    sheet.write(i+3,2,ji2[i])\n",
    "    sheet.write(i+3,3,abs(ji3[i]-ji2[i]))\n",
    "book.save(\"回归预测比特币.xls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**时间序列**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn import linear_model\n",
    "# from d2l import torch as d2l\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f=open('lstm比特币预测.csv','w',encoding='utf-8',newline=\"\")\n",
    "csv_writer=csv.writer(f)\n",
    "csv_writer.writerow([\"实际价格\",\"预测价格\"])\n",
    "for i in range(0,input_size-100+1):\n",
    "    tmp=[]\n",
    "    tmp.append(actuals[i])\n",
    "    tmp.append(round(predictions[i],2))\n",
    "    csv_writer.writerow(tmp)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**灰度预测**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def GM11(x,n):\n",
    "    '''\n",
    "    灰色预测\n",
    "    x：序列，numpy对象\n",
    "    n:需要往后预测的个数\n",
    "    '''\n",
    "    x1 = x.cumsum()#一次累加\n",
    "    z1 = (x1[:len(x1) - 1] + x1[1:])/2.0#紧邻均值\n",
    "    z1 = z1.reshape((len(z1),1))\n",
    "    B = np.append(-z1,np.ones_like(z1),axis=1)\n",
    "    Y = x[1:].reshape((len(x) - 1,1))\n",
    "    #a为发展系数 b为灰色作用量\n",
    "    [[a],[b]] = np.dot(np.dot(np.linalg.inv(np.dot(B.T, B)), B.T), Y)#计算参数\n",
    "    result = (x[0]-b/a)*np.exp(-a*(n-1))-(x[0]-b/a)*np.exp(-a*(n-2))\n",
    "    S1_2 = x.var()#原序列方差\n",
    "    e = list()#残差序列\n",
    "    for index in range(1,x.shape[0]+1):\n",
    "        predict = (x[0]-b/a)*np.exp(-a*(index-1))-(x[0]-b/a)*np.exp(-a*(index-2))\n",
    "        e.append(x[index-1]-predict)\n",
    "    S2_2 = np.array(e).var()#残差方差\n",
    "    C = S2_2/S1_2#后验差比\n",
    "    if C<=0.35:\n",
    "        assess = '后验差比<=0.35，模型精度等级为好'\n",
    "    elif C<=0.5:\n",
    "        assess = '后验差比<=0.5，模型精度等级为合格'\n",
    "    elif C<=0.65:\n",
    "        assess = '后验差比<=0.65，模型精度等级为勉强'\n",
    "    else:\n",
    "        assess = '后验差比>0.65，模型精度等级为不合格'\n",
    "    #预测数据\n",
    "    predict = list()\n",
    "    for index in range(x.shape[0]+1,x.shape[0]+n+1):\n",
    "        predict.append((x[0]-b/a)*np.exp(-a*(index-1))-(x[0]-b/a)*np.exp(-a*(index-2)))\n",
    "    predict = np.array(predict)\n",
    "    return {\n",
    "            'a':{'value':a,'desc':'发展系数'},\n",
    "            'b':{'value':b,'desc':'灰色作用量'},\n",
    "            'predict':{'value':result,'desc':'第%d个预测值'%n},\n",
    "            'C':{'value':C,'desc':assess},\n",
    "            'predict':{'value':predict,'desc':'往后预测%d个的序列'%(n)},\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[621.65, 0], [609.67, 0], [610.92, 0]]\n"
     ]
    }
   ],
   "source": [
    "# data = np.array([1.2,2.2,3.1,4.5,5.6,6.7,7.1,8.2,9.6,10.6,11,12.4,13.5,14.7,15.2])\n",
    "# x = data[0:10]#输入数据\n",
    "# y = data[10:11]#需要预测的数据\n",
    "# result = GM11(x,len(y))\n",
    "# predict = result['predict']['value']\n",
    "# predict = np.round(predict,1)\n",
    "# print('真实值:',y)\n",
    "# print('预测值:',predict)\n",
    "# print(result)\n",
    "\n",
    "\n",
    "# #首先进行比特币的灰度预测\n",
    "df=pd.read_csv(r\"C:\\Users\\rcz\\Desktop\\运筹学\\代码及数据\\1 数据预处理\\处理后数据\\C题处理后的中间文件.csv\")\n",
    "train=[]\n",
    "height,weight=df.shape\n",
    "# height=10\n",
    "for i in range(0,height):\n",
    "    train.append(df.values[i][1])\n",
    "train=np.array(train)\n",
    "# print(train)\n",
    "res=[]\n",
    "tmp=[]\n",
    "tmp.append(train[0])\n",
    "tmp.append(0)\n",
    "res.append(tmp)\n",
    "tmp=[]\n",
    "tmp.append(train[1])\n",
    "tmp.append(0)\n",
    "res.append(tmp)\n",
    "tmp=[]\n",
    "tmp.append(train[2])\n",
    "tmp.append(0)\n",
    "res.append(tmp)\n",
    "print(res)\n",
    "for i in range(2,height-1):\n",
    "    x=train[0:i+1]\n",
    "    y=train[i+1:i+2]\n",
    "    result = GM11(x,len(y))\n",
    "    predict = result['predict']['value']\n",
    "    predict = np.round(predict,1)\n",
    "    tmp=[]\n",
    "    tmp.append(predict[0])\n",
    "    tmp.append(round(abs(predict[0]-train[i+1]),2))\n",
    "    # print('真实值:',y)\n",
    "    # print('预测值:',predict)\n",
    "    # print(result)\n",
    "    res.append(tmp)\n",
    "\n",
    "# #计算误差\n",
    "# for i in range(0,height):\n",
    "#     wucha1=abs(res[i]-train[i])\n",
    "#     wucha.append(wucha1)\n",
    "\n",
    "# print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#将其保存在csv文件中终\n",
    "print(res)\n",
    "f = open('灰度预测比特币.csv', 'w', encoding='utf-8', newline=\"\")\n",
    "csv_writer = csv.writer(f)\n",
    "csv_writer.writerow([\"预测结果\",\"误差\"])\n",
    "for i in range(0,height):\n",
    "    csv_writer.writerow(res[i])\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}